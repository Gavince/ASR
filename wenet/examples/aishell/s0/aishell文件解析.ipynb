{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "126a501a-537f-43f8-a2c1-1beece58fca5",
   "metadata": {},
   "source": [
    "# WeNe之aishell数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62cb4d81-30f9-44fd-9a9f-f6bd847889ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "import torchaudio\n",
    "import torch.distributed as dist\n",
    "import random\n",
    "import yaml\n",
    "import torchaudio.compliance.kaldi as kaldi\n",
    "from wenet.utils.file_utils import read_lists\n",
    "import wenet.dataset.processor as processor\n",
    "from wenet.utils.file_utils import read_symbol_table, read_non_lang_symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27483b6-5d55-4dfe-9cc5-ecdbdd12d19e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 获取数据配置文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c38de90-abaa-4997-b078-6994c59dab6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = r'conf/train_conformer.yaml'\n",
    "\n",
    "with open(path, 'r') as fin:\n",
    "    configs = yaml.load(fin, Loader=yaml.FullLoader)\n",
    "\n",
    "conf = configs[\"dataset_conf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a700f8c-a545-4c47-a5d3-f1f0bed62077",
   "metadata": {},
   "source": [
    "## 获取Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef2476a5-3a78-4465-bfc2-aedc9e1459f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DistributedSampler:\n",
    "    def __init__(self, shuffle=True, partition=True):\n",
    "        self.epoch = -1\n",
    "        self.update()\n",
    "        self.shuffle = shuffle\n",
    "        self.partition = partition\n",
    "\n",
    "    def update(self):\n",
    "        assert dist.is_available()\n",
    "        if dist.is_initialized():\n",
    "            self.rank = dist.get_rank()\n",
    "            self.world_size = dist.get_world_size()\n",
    "        else:\n",
    "            self.rank = 0\n",
    "            self.world_size = 1\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            self.worker_id = 0\n",
    "            self.num_workers = 1\n",
    "        else:\n",
    "            self.worker_id = worker_info.id\n",
    "            self.num_workers = worker_info.num_workers\n",
    "        return dict(rank=self.rank,\n",
    "                    world_size=self.world_size,\n",
    "                    worker_id=self.worker_id,\n",
    "                    num_workers=self.num_workers)\n",
    "    \n",
    "    def sample(self, data):\n",
    "        \"\"\" Sample data according to rank/world_size/num_workers\n",
    "\n",
    "            Args:\n",
    "                data(List): input data list\n",
    "\n",
    "            Returns:\n",
    "                List: data list after sample\n",
    "        \"\"\"\n",
    "        # 采样后信息\n",
    "        data = list(range(len(data)))\n",
    "        # TODO(Binbin Zhang): fix this\n",
    "        # We can not handle uneven data for CV on DDP, so we don't\n",
    "        # sample data by rank, that means every GPU gets the same\n",
    "        # and all the CV data\n",
    "        if self.partition:\n",
    "            if self.shuffle:\n",
    "                random.Random(self.epoch).shuffle(data)\n",
    "            data = data[self.rank::self.world_size]\n",
    "        data = data[self.worker_id::self.num_workers]\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb53df1e-21a4-4de3-85be-3968221cd31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataList(IterableDataset):\n",
    "    \"\"\"完成大内存数据读取\n",
    "    \"\"\"\n",
    "    def __init__(self, lists, shuffle=True, partition=True):\n",
    "        self.lists = lists\n",
    "        self.sampler = DistributedSampler(shuffle, partition)\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.sampler.set_epoch(epoch)\n",
    "\n",
    "    def __iter__(self):\n",
    "        sampler_info = self.sampler.update()\n",
    "        indexes = self.sampler.sample(self.lists)\n",
    "        for index in indexes:\n",
    "            # yield dict(src=src)\n",
    "            data = dict(src=self.lists[index])\n",
    "            data.update(sampler_info)\n",
    "            yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f2e55d8-4c13-4aa8-a7e7-421993d616a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_lists(list_file):\n",
    "    lists = []\n",
    "    with open(list_file, 'r', encoding='utf8') as fin:\n",
    "        for line in fin:\n",
    "            lists.append(line.strip())\n",
    "    return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e549515d-2bff-4dc5-8c3c-c96f74045f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Processor(IterableDataset):\n",
    "    def __init__(self, source, f, *args, **kw):\n",
    "        assert callable(f)\n",
    "        self.source = source\n",
    "        self.f = f\n",
    "        self.args = args\n",
    "        self.kw = kw\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.source.set_epoch(epoch)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\" Return an iterator over the source dataset processed by the\n",
    "            given processor.\n",
    "        \"\"\"\n",
    "        assert self.source is not None\n",
    "        assert callable(self.f)\n",
    "        return self.f(iter(self.source), *self.args, **self.kw)\n",
    "\n",
    "    def apply(self, f):\n",
    "        assert callable(f)\n",
    "        return Processor(self, f, *self.args, **self.kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e081e100-b67b-451e-9962-26a28f022838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_data = read_lists(\"./data/train/data.list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8188c8-c436-46f8-a51d-f78fb80f4ac0",
   "metadata": {},
   "source": [
    "## 预处理操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc6f61-cd6b-4bbc-a528-f735bc633df1",
   "metadata": {},
   "source": [
    "### 读取原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8908e405-96e9-402e-88f5-9ed5f92a3284",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = DataList(list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c11b50a4-2565-4b00-89aa-c717647fcccf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': '{\"key\": \"BAC009S0160W0199\", \"wav\": \"/home/gavin/Machine/data/asr/aishell/data_aishell/wav/train/S0160/BAC009S0160W0199.wav\", \"txt\": \"最大的问题就是提供了某种隐私的背书或担保\"}', 'rank': 0, 'world_size': 1, 'worker_id': 0, 'num_workers': 1}\n",
      "{'src': '{\"key\": \"BAC009S0079W0181\", \"wav\": \"/home/gavin/Machine/data/asr/aishell/data_aishell/wav/train/S0079/BAC009S0079W0181.wav\", \"txt\": \"几天前有媒体曝光了这片别墅\"}', 'rank': 0, 'world_size': 1, 'worker_id': 0, 'num_workers': 1}\n",
      "{'src': '{\"key\": \"BAC009S0114W0438\", \"wav\": \"/home/gavin/Machine/data/asr/aishell/data_aishell/wav/train/S0114/BAC009S0114W0438.wav\", \"txt\": \"通州法院判决驳回了潘老太的诉求\"}', 'rank': 0, 'world_size': 1, 'worker_id': 0, 'num_workers': 1}\n",
      "{'src': '{\"key\": \"BAC009S0096W0254\", \"wav\": \"/home/gavin/Machine/data/asr/aishell/data_aishell/wav/train/S0096/BAC009S0096W0254.wav\", \"txt\": \"未来科技必须回到以人为中心\"}', 'rank': 0, 'world_size': 1, 'worker_id': 0, 'num_workers': 1}\n",
      "{'src': '{\"key\": \"BAC009S0349W0190\", \"wav\": \"/home/gavin/Machine/data/asr/aishell/data_aishell/wav/train/S0349/BAC009S0349W0190.wav\", \"txt\": \"产权市场纷纷抢食这块蛋糕\"}', 'rank': 0, 'world_size': 1, 'worker_id': 0, 'num_workers': 1}\n",
      "{'src': '{\"key\": \"BAC009S0422W0354\", \"wav\": \"/home/gavin/Machine/data/asr/aishell/data_aishell/wav/train/S0422/BAC009S0422W0354.wav\", \"txt\": \"由西向东步行约一二十米左右\"}', 'rank': 0, 'world_size': 1, 'worker_id': 0, 'num_workers': 1}\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataset):\n",
    "    print(data) \n",
    "    if i == 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e720e9e-2b09-4801-a168-1c94bf511e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = Processor(dataset, processor.parse_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7da42d22-2300-4ab7-adbd-60642e6c75bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'BAC009S0160W0199', 'txt': '最大的问题就是提供了某种隐私的背书或担保', 'wav': tensor([[-0.0023, -0.0039, -0.0035,  ..., -0.0048, -0.0050, -0.0044]]), 'sample_rate': 16000}\n",
      "{'key': 'BAC009S0079W0181', 'txt': '几天前有媒体曝光了这片别墅', 'wav': tensor([[-0.0020, -0.0033, -0.0029,  ..., -0.0013, -0.0013, -0.0015]]), 'sample_rate': 16000}\n",
      "{'key': 'BAC009S0114W0438', 'txt': '通州法院判决驳回了潘老太的诉求', 'wav': tensor([[-0.0004, -0.0005, -0.0002,  ...,  0.0002,  0.0003,  0.0003]]), 'sample_rate': 16000}\n",
      "{'key': 'BAC009S0096W0254', 'txt': '未来科技必须回到以人为中心', 'wav': tensor([[ 0.0004,  0.0007,  0.0008,  ..., -0.0008, -0.0007, -0.0006]]), 'sample_rate': 16000}\n",
      "{'key': 'BAC009S0349W0190', 'txt': '产权市场纷纷抢食这块蛋糕', 'wav': tensor([[0.0004, 0.0007, 0.0007,  ..., 0.0005, 0.0004, 0.0006]]), 'sample_rate': 16000}\n",
      "{'key': 'BAC009S0422W0354', 'txt': '由西向东步行约一二十米左右', 'wav': tensor([[-0.0013, -0.0017, -0.0008,  ...,  0.0037,  0.0034,  0.0041]]), 'sample_rate': 16000}\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataset):\n",
    "    print(data) \n",
    "    if i == 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf90e0-432f-4667-b9f9-9058327abb6d",
   "metadata": {},
   "source": [
    "### 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02338056-649c-44d2-9041-87ca7c7593e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_lang_syms = read_non_lang_symbols(non_lang_sym_path=None)\n",
    "symbol_table = read_symbol_table(\"./data/dict/lang_char.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbdb88ab-2ebe-4158-ba54-e33c6bc7b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Processor(dataset, processor.tokenize, symbol_table, None,\n",
    "                    non_lang_syms, conf.get('split_with_space', False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96a2cf32-2b08-4afd-8675-e52d5af457c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'BAC009S0160W0199', 'txt': '最大的问题就是提供了某种隐私的背书或担保', 'wav': tensor([[-0.0023, -0.0039, -0.0035,  ..., -0.0048, -0.0050, -0.0044]]), 'sample_rate': 16000, 'tokens': ['最', '大', '的', '问', '题', '就', '是', '提', '供', '了', '某', '种', '隐', '私', '的', '背', '书', '或', '担', '保'], 'label': [1740, 814, 2553, 3925, 4077, 1030, 1694, 1556, 184, 66, 1806, 2723, 3984, 2718, 2553, 3047, 61, 1382, 1450, 205]}\n",
      "{'key': 'BAC009S0079W0181', 'txt': '几天前有媒体曝光了这片别墅', 'wav': tensor([[-0.0020, -0.0033, -0.0029,  ..., -0.0013, -0.0013, -0.0015]]), 'sample_rate': 16000, 'tokens': ['几', '天', '前', '有', '媒', '体', '曝', '光', '了', '这', '片', '别', '墅'], 'label': [321, 815, 366, 1742, 916, 162, 1731, 262, 66, 3703, 2339, 353, 787]}\n",
      "{'key': 'BAC009S0114W0438', 'txt': '通州法院判决驳回了潘老太的诉求', 'wav': tensor([[-0.0004, -0.0005, -0.0002,  ...,  0.0002,  0.0003,  0.0003]]), 'sample_rate': 16000, 'tokens': ['通', '州', '法', '院', '判', '决', '驳', '回', '了', '潘', '老', '太', '的', '诉', '求'], 'label': [3733, 1104, 2049, 3970, 350, 306, 4125, 703, 66, 2222, 2990, 816, 2553, 3465, 1995]}\n",
      "{'key': 'BAC009S0096W0254', 'txt': '未来科技必须回到以人为中心', 'wav': tensor([[ 0.0004,  0.0007,  0.0008,  ..., -0.0008, -0.0007, -0.0006]]), 'sample_rate': 16000, 'tokens': ['未', '来', '科', '技', '必', '须', '回', '到', '以', '人', '为', '中', '心'], 'label': [1753, 1778, 2724, 1422, 1246, 4059, 703, 355, 117, 95, 36, 30, 1245]}\n",
      "{'key': 'BAC009S0349W0190', 'txt': '产权市场纷纷抢食这块蛋糕', 'wav': tensor([[0.0004, 0.0007, 0.0007,  ..., 0.0005, 0.0004, 0.0006]]), 'sample_rate': 16000, 'tokens': ['产', '权', '市', '场', '纷', '纷', '抢', '食', '这', '块', '蛋', '糕'], 'label': [85, 1765, 1121, 724, 2880, 2880, 1437, 4090, 3703, 735, 3322, 2849]}\n",
      "{'key': 'BAC009S0422W0354', 'txt': '由西向东步行约一二十米左右', 'wav': tensor([[-0.0013, -0.0017, -0.0008,  ...,  0.0037,  0.0034,  0.0041]]), 'sample_rate': 16000, 'tokens': ['由', '西', '向', '东', '步', '行', '约', '一', '二', '十', '米', '左', '右'], 'label': [2483, 3413, 528, 21, 1940, 3363, 2869, 2, 70, 426, 2832, 1108, 509]}\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataset):\n",
    "    print(data) \n",
    "    if i == 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f370ef-f2f8-4aac-b55a-c6f4e3938a93",
   "metadata": {},
   "source": [
    "### 过滤与重采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9974bc12-82ee-4126-9eb9-d1ccd3e9e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_conf = conf.get('filter_conf', {})\n",
    "dataset = Processor(dataset, processor.filter, **filter_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68ead222-8ad3-4bed-b1d5-bcf5eb37aa6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'BAC009S0160W0199', 'txt': '最大的问题就是提供了某种隐私的背书或担保', 'wav': tensor([[-0.0023, -0.0039, -0.0035,  ..., -0.0048, -0.0050, -0.0044]]), 'sample_rate': 16000, 'tokens': ['最', '大', '的', '问', '题', '就', '是', '提', '供', '了', '某', '种', '隐', '私', '的', '背', '书', '或', '担', '保'], 'label': [1740, 814, 2553, 3925, 4077, 1030, 1694, 1556, 184, 66, 1806, 2723, 3984, 2718, 2553, 3047, 61, 1382, 1450, 205]}\n",
      "{'key': 'BAC009S0079W0181', 'txt': '几天前有媒体曝光了这片别墅', 'wav': tensor([[-0.0020, -0.0033, -0.0029,  ..., -0.0013, -0.0013, -0.0015]]), 'sample_rate': 16000, 'tokens': ['几', '天', '前', '有', '媒', '体', '曝', '光', '了', '这', '片', '别', '墅'], 'label': [321, 815, 366, 1742, 916, 162, 1731, 262, 66, 3703, 2339, 353, 787]}\n",
      "{'key': 'BAC009S0114W0438', 'txt': '通州法院判决驳回了潘老太的诉求', 'wav': tensor([[-0.0004, -0.0005, -0.0002,  ...,  0.0002,  0.0003,  0.0003]]), 'sample_rate': 16000, 'tokens': ['通', '州', '法', '院', '判', '决', '驳', '回', '了', '潘', '老', '太', '的', '诉', '求'], 'label': [3733, 1104, 2049, 3970, 350, 306, 4125, 703, 66, 2222, 2990, 816, 2553, 3465, 1995]}\n",
      "{'key': 'BAC009S0096W0254', 'txt': '未来科技必须回到以人为中心', 'wav': tensor([[ 0.0004,  0.0007,  0.0008,  ..., -0.0008, -0.0007, -0.0006]]), 'sample_rate': 16000, 'tokens': ['未', '来', '科', '技', '必', '须', '回', '到', '以', '人', '为', '中', '心'], 'label': [1753, 1778, 2724, 1422, 1246, 4059, 703, 355, 117, 95, 36, 30, 1245]}\n",
      "{'key': 'BAC009S0349W0190', 'txt': '产权市场纷纷抢食这块蛋糕', 'wav': tensor([[0.0004, 0.0007, 0.0007,  ..., 0.0005, 0.0004, 0.0006]]), 'sample_rate': 16000, 'tokens': ['产', '权', '市', '场', '纷', '纷', '抢', '食', '这', '块', '蛋', '糕'], 'label': [85, 1765, 1121, 724, 2880, 2880, 1437, 4090, 3703, 735, 3322, 2849]}\n",
      "{'key': 'BAC009S0422W0354', 'txt': '由西向东步行约一二十米左右', 'wav': tensor([[-0.0013, -0.0017, -0.0008,  ...,  0.0037,  0.0034,  0.0041]]), 'sample_rate': 16000, 'tokens': ['由', '西', '向', '东', '步', '行', '约', '一', '二', '十', '米', '左', '右'], 'label': [2483, 3413, 528, 21, 1940, 3363, 2869, 2, 70, 426, 2832, 1108, 509]}\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataset):\n",
    "    print(data) \n",
    "    if i == 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc21ce4c-4eb4-4466-b596-0ff49c4a7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_conf = conf.get('resample_conf', {})\n",
    "dataset = Processor(dataset, processor.resample, **resample_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ad601b4-c441-41d2-83f5-99c8aa90d9c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'BAC009S0160W0199', 'txt': '最大的问题就是提供了某种隐私的背书或担保', 'wav': tensor([[-0.0023, -0.0039, -0.0035,  ..., -0.0048, -0.0050, -0.0044]]), 'sample_rate': 16000, 'tokens': ['最', '大', '的', '问', '题', '就', '是', '提', '供', '了', '某', '种', '隐', '私', '的', '背', '书', '或', '担', '保'], 'label': [1740, 814, 2553, 3925, 4077, 1030, 1694, 1556, 184, 66, 1806, 2723, 3984, 2718, 2553, 3047, 61, 1382, 1450, 205]}\n",
      "{'key': 'BAC009S0079W0181', 'txt': '几天前有媒体曝光了这片别墅', 'wav': tensor([[-0.0020, -0.0033, -0.0029,  ..., -0.0013, -0.0013, -0.0015]]), 'sample_rate': 16000, 'tokens': ['几', '天', '前', '有', '媒', '体', '曝', '光', '了', '这', '片', '别', '墅'], 'label': [321, 815, 366, 1742, 916, 162, 1731, 262, 66, 3703, 2339, 353, 787]}\n",
      "{'key': 'BAC009S0114W0438', 'txt': '通州法院判决驳回了潘老太的诉求', 'wav': tensor([[-0.0004, -0.0005, -0.0002,  ...,  0.0002,  0.0003,  0.0003]]), 'sample_rate': 16000, 'tokens': ['通', '州', '法', '院', '判', '决', '驳', '回', '了', '潘', '老', '太', '的', '诉', '求'], 'label': [3733, 1104, 2049, 3970, 350, 306, 4125, 703, 66, 2222, 2990, 816, 2553, 3465, 1995]}\n",
      "{'key': 'BAC009S0096W0254', 'txt': '未来科技必须回到以人为中心', 'wav': tensor([[ 0.0004,  0.0007,  0.0008,  ..., -0.0008, -0.0007, -0.0006]]), 'sample_rate': 16000, 'tokens': ['未', '来', '科', '技', '必', '须', '回', '到', '以', '人', '为', '中', '心'], 'label': [1753, 1778, 2724, 1422, 1246, 4059, 703, 355, 117, 95, 36, 30, 1245]}\n",
      "{'key': 'BAC009S0349W0190', 'txt': '产权市场纷纷抢食这块蛋糕', 'wav': tensor([[0.0004, 0.0007, 0.0007,  ..., 0.0005, 0.0004, 0.0006]]), 'sample_rate': 16000, 'tokens': ['产', '权', '市', '场', '纷', '纷', '抢', '食', '这', '块', '蛋', '糕'], 'label': [85, 1765, 1121, 724, 2880, 2880, 1437, 4090, 3703, 735, 3322, 2849]}\n",
      "{'key': 'BAC009S0422W0354', 'txt': '由西向东步行约一二十米左右', 'wav': tensor([[-0.0013, -0.0017, -0.0008,  ...,  0.0037,  0.0034,  0.0041]]), 'sample_rate': 16000, 'tokens': ['由', '西', '向', '东', '步', '行', '约', '一', '二', '十', '米', '左', '右'], 'label': [2483, 3413, 528, 21, 1940, 3363, 2869, 2, 70, 426, 2832, 1108, 509]}\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataset):\n",
    "    print(data) \n",
    "    if i == 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23e2b586-962f-4dfe-8af5-c57b49521a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filter_conf': {'max_length': 40960,\n",
       "  'min_length': 0,\n",
       "  'token_max_length': 200,\n",
       "  'token_min_length': 1},\n",
       " 'resample_conf': {'resample_rate': 16000},\n",
       " 'speed_perturb': True,\n",
       " 'fbank_conf': {'num_mel_bins': 80,\n",
       "  'frame_shift': 10,\n",
       "  'frame_length': 25,\n",
       "  'dither': 0.1},\n",
       " 'spec_aug': True,\n",
       " 'spec_aug_conf': {'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10},\n",
       " 'shuffle': True,\n",
       " 'shuffle_conf': {'shuffle_size': 1500},\n",
       " 'sort': True,\n",
       " 'sort_conf': {'sort_size': 500},\n",
       " 'batch_conf': {'batch_type': 'static', 'batch_size': 2}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5846c98c-2f47-4075-9d67-4be839b7fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_perturb = conf.get('speed_perturb', False)\n",
    "if speed_perturb:\n",
    "    dataset = Processor(dataset, processor.speed_perturb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed5a9eee-8a27-47b5-9cd9-7bce5a2b4890",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'BAC009S0160W0199', 'txt': '最大的问题就是提供了某种隐私的背书或担保', 'wav': tensor([[-0.0022, -0.0039, -0.0035,  ..., -0.0048, -0.0049, -0.0050]]), 'sample_rate': 16000, 'tokens': ['最', '大', '的', '问', '题', '就', '是', '提', '供', '了', '某', '种', '隐', '私', '的', '背', '书', '或', '担', '保'], 'label': [1740, 814, 2553, 3925, 4077, 1030, 1694, 1556, 184, 66, 1806, 2723, 3984, 2718, 2553, 3047, 61, 1382, 1450, 205]}\n",
      "{'key': 'BAC009S0079W0181', 'txt': '几天前有媒体曝光了这片别墅', 'wav': tensor([[-0.0019, -0.0033, -0.0029,  ..., -0.0014, -0.0012, -0.0016]]), 'sample_rate': 16000, 'tokens': ['几', '天', '前', '有', '媒', '体', '曝', '光', '了', '这', '片', '别', '墅'], 'label': [321, 815, 366, 1742, 916, 162, 1731, 262, 66, 3703, 2339, 353, 787]}\n",
      "{'key': 'BAC009S0114W0438', 'txt': '通州法院判决驳回了潘老太的诉求', 'wav': tensor([[-0.0004, -0.0005, -0.0002,  ...,  0.0002,  0.0003,  0.0003]]), 'sample_rate': 16000, 'tokens': ['通', '州', '法', '院', '判', '决', '驳', '回', '了', '潘', '老', '太', '的', '诉', '求'], 'label': [3733, 1104, 2049, 3970, 350, 306, 4125, 703, 66, 2222, 2990, 816, 2553, 3465, 1995]}\n",
      "{'key': 'BAC009S0096W0254', 'txt': '未来科技必须回到以人为中心', 'wav': tensor([[ 0.0004,  0.0007,  0.0008,  ..., -0.0008, -0.0007, -0.0006]]), 'sample_rate': 16000, 'tokens': ['未', '来', '科', '技', '必', '须', '回', '到', '以', '人', '为', '中', '心'], 'label': [1753, 1778, 2724, 1422, 1246, 4059, 703, 355, 117, 95, 36, 30, 1245]}\n",
      "{'key': 'BAC009S0349W0190', 'txt': '产权市场纷纷抢食这块蛋糕', 'wav': tensor([[0.0004, 0.0007, 0.0007,  ..., 0.0004, 0.0006, 0.0004]]), 'sample_rate': 16000, 'tokens': ['产', '权', '市', '场', '纷', '纷', '抢', '食', '这', '块', '蛋', '糕'], 'label': [85, 1765, 1121, 724, 2880, 2880, 1437, 4090, 3703, 735, 3322, 2849]}\n",
      "{'key': 'BAC009S0422W0354', 'txt': '由西向东步行约一二十米左右', 'wav': tensor([[-0.0013, -0.0017, -0.0009,  ...,  0.0033,  0.0040,  0.0034]]), 'sample_rate': 16000, 'tokens': ['由', '西', '向', '东', '步', '行', '约', '一', '二', '十', '米', '左', '右'], 'label': [2483, 3413, 528, 21, 1940, 3363, 2869, 2, 70, 426, 2832, 1108, 509]}\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataset):\n",
    "    print(data) \n",
    "    if i == 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd2b3854-2aa9-4f7f-9112-ce5ccfe2573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取音频特征\n",
    "feats_type = conf.get('feats_type', 'fbank')\n",
    "assert feats_type in ['fbank', 'mfcc']\n",
    "if feats_type == 'fbank':\n",
    "    fbank_conf = conf.get('fbank_conf', {})\n",
    "    dataset = Processor(dataset, processor.compute_fbank, **fbank_conf)\n",
    "elif feats_type == 'mfcc':\n",
    "    mfcc_conf = conf.get('mfcc_conf', {})\n",
    "    dataset = Processor(dataset, processor.compute_mfcc, **mfcc_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a72dc26-5c4b-41ed-99aa-21867b941700",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'BAC009S0160W0199', 'label': [1740, 814, 2553, 3925, 4077, 1030, 1694, 1556, 184, 66, 1806, 2723, 3984, 2718, 2553, 3047, 61, 1382, 1450, 205], 'feat': tensor([[10.0034,  9.4124,  8.8504,  ...,  3.1760,  3.1335,  3.3529],\n",
      "        [11.0576, 10.7693,  8.4619,  ...,  3.4684,  4.1584,  4.2449],\n",
      "        [ 9.8402,  9.7405,  8.2489,  ...,  3.7678,  3.1555,  3.3836],\n",
      "        ...,\n",
      "        [11.9243, 11.3285,  7.8447,  ...,  3.8865,  3.7032,  3.0156],\n",
      "        [12.3342, 11.8614,  9.1529,  ...,  3.5955,  4.2694,  3.5337],\n",
      "        [10.3848, 10.0204,  7.7809,  ...,  3.2788,  3.5594,  4.1530]])}\n",
      "{'key': 'BAC009S0079W0181', 'label': [321, 815, 366, 1742, 916, 162, 1731, 262, 66, 3703, 2339, 353, 787], 'feat': tensor([[10.7570, 10.7426,  8.2759,  ..., 10.0635,  9.1715,  8.3663],\n",
      "        [10.2884,  9.9882,  8.2937,  ...,  9.4569,  9.8060,  8.3763],\n",
      "        [ 7.2598,  7.7229,  7.4193,  ...,  9.2461,  8.6847,  7.8356],\n",
      "        ...,\n",
      "        [ 8.2552,  8.0596,  7.2004,  ...,  8.8967,  9.4172,  7.6320],\n",
      "        [10.1915, 10.1922,  8.4978,  ...,  8.8424,  8.9059,  7.5462],\n",
      "        [ 8.7046,  6.9722,  7.7000,  ...,  9.7719,  9.2547,  7.8436]])}\n",
      "{'key': 'BAC009S0114W0438', 'label': [3733, 1104, 2049, 3970, 350, 306, 4125, 703, 66, 2222, 2990, 816, 2553, 3465, 1995], 'feat': tensor([[ 7.1860,  7.3206,  2.8748,  ...,  9.3968,  9.0480,  8.7720],\n",
      "        [ 7.8690,  8.2483,  6.8417,  ..., 10.2105,  9.5849,  8.5205],\n",
      "        [ 5.0079,  5.5601,  6.9100,  ..., 10.7185,  9.8889,  7.9336],\n",
      "        ...,\n",
      "        [ 8.2189,  8.6656,  7.4203,  ...,  9.9159,  8.7359,  7.9387],\n",
      "        [ 7.8794,  8.1774,  6.6620,  ...,  9.6132,  8.9849,  7.8499],\n",
      "        [ 6.4664,  7.1544,  6.4311,  ...,  9.6462,  9.1730,  8.2352]])}\n",
      "{'key': 'BAC009S0096W0254', 'label': [1753, 1778, 2724, 1422, 1246, 4059, 703, 355, 117, 95, 36, 30, 1245], 'feat': tensor([[ 9.3053,  9.4942,  8.0200,  ...,  9.7537,  9.0812,  8.1207],\n",
      "        [ 8.6455,  8.6632,  7.3245,  ...,  8.8216,  9.2700,  8.1955],\n",
      "        [ 8.2900,  7.4496,  8.0417,  ...,  9.1545,  9.3210,  8.3034],\n",
      "        ...,\n",
      "        [ 5.2952,  6.6413,  7.6299,  ..., 10.2062,  9.1192,  7.8090],\n",
      "        [ 7.0301,  7.2678,  7.6515,  ...,  9.7998, 10.0128,  8.9981],\n",
      "        [ 7.8415,  8.7284,  8.6992,  ...,  9.8094,  9.8022,  8.6657]])}\n",
      "{'key': 'BAC009S0349W0190', 'label': [85, 1765, 1121, 724, 2880, 2880, 1437, 4090, 3703, 735, 3322, 2849], 'feat': tensor([[ 7.9158,  8.1685,  9.1120,  ..., 11.5381, 11.5665,  9.2339],\n",
      "        [11.0075, 10.8652,  6.9449,  ..., 11.8333, 11.3139,  9.6336],\n",
      "        [ 9.9340,  9.7016,  9.0719,  ..., 11.2653, 11.0174,  9.1163],\n",
      "        ...,\n",
      "        [10.8354,  9.7284,  9.1108,  ..., 11.7391, 11.3274,  9.0761],\n",
      "        [10.2789,  8.7193,  9.5905,  ..., 11.8233, 11.3117,  9.5479],\n",
      "        [ 8.9152,  9.2434,  8.7702,  ..., 11.5368, 10.6427,  9.4115]])}\n",
      "{'key': 'BAC009S0422W0354', 'label': [2483, 3413, 528, 21, 1940, 3363, 2869, 2, 70, 426, 2832, 1108, 509], 'feat': tensor([[ 9.4107,  9.3253,  7.2993,  ..., 10.7975, 10.5837,  8.8481],\n",
      "        [10.6929, 10.7725,  9.3725,  ..., 11.0860, 11.6390,  9.7431],\n",
      "        [ 8.6536,  8.4090,  9.2644,  ..., 11.0168, 11.7673, 10.2792],\n",
      "        ...,\n",
      "        [12.9847, 13.1999, 11.0004,  ...,  8.9185,  9.5159,  8.6365],\n",
      "        [12.7617, 12.8513, 10.2769,  ..., 10.2349, 10.7841,  9.1816],\n",
      "        [12.0491, 12.0664, 10.5254,  ..., 10.0376, 10.1734,  8.1079]])}\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataset):\n",
    "    print(data) \n",
    "    if i == 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc22e842-0237-4b1c-a1ae-fdd66d05db94",
   "metadata": {},
   "source": [
    "### 音频特征增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5a3d78b-13b7-4c4d-8147-194c95e8ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_aug = conf.get('spec_aug', True)\n",
    "spec_sub = conf.get('spec_sub', False)\n",
    "spec_trim = conf.get('spec_trim', False)\n",
    "shuffle = conf.get('shuffle', True)\n",
    "if spec_aug:\n",
    "    spec_aug_conf = conf.get('spec_aug_conf', {})\n",
    "    dataset = Processor(dataset, processor.spec_aug, **spec_aug_conf)\n",
    "if spec_sub:\n",
    "    spec_sub_conf = conf.get('spec_sub_conf', {})\n",
    "    dataset = Processor(dataset, processor.spec_sub, **spec_sub_conf)\n",
    "if spec_trim:\n",
    "    spec_trim_conf = conf.get('spec_trim_conf', {})\n",
    "    dataset = Processor(dataset, processor.spec_trim, **spec_trim_conf)\n",
    "\n",
    "if shuffle:\n",
    "    shuffle_conf = conf.get('shuffle_conf', {})\n",
    "    dataset = Processor(dataset, processor.shuffle, **shuffle_conf)\n",
    "\n",
    "sort = conf.get('sort', True)\n",
    "if sort:\n",
    "    sort_conf = conf.get('sort_conf', {})\n",
    "    dataset = Processor(dataset, processor.sort, **sort_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5735c521-e3be-4e6e-bec6-498f689498b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'BAC009S0026W0411', 'label': [25, 95, 3014, 524, 62, 271], 'feat': tensor([[ 8.6380,  9.4849,  8.7528,  ..., 10.0332,  9.4069,  8.3837],\n",
      "        [ 9.7140,  9.2929,  6.8356,  ...,  9.2110,  9.1941,  7.8953],\n",
      "        [ 9.6509,  9.4898,  9.3455,  ..., 10.0294,  9.5593,  8.1604],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])}\n",
      "{'key': 'BAC009S0128W0267', 'label': [25, 427, 75, 2551, 70, 426, 56], 'feat': tensor([[ 8.1434,  8.3520,  6.8038,  ...,  4.2025,  3.3710,  3.6203],\n",
      "        [ 9.1759,  9.1142,  7.5651,  ...,  3.1498,  3.5437,  3.6296],\n",
      "        [10.0043,  9.9018,  7.5152,  ...,  3.3204,  2.4338,  3.4348],\n",
      "        ...,\n",
      "        [ 8.0451,  7.1918,  6.5447,  ...,  3.7069,  4.1118,  3.2407],\n",
      "        [ 9.2800,  9.4476,  6.9969,  ...,  3.1703,  3.7083,  4.9803],\n",
      "        [ 7.5782,  6.7601,  5.9846,  ...,  3.9763,  3.8132,  4.2691]])}\n",
      "{'key': 'BAC009S0189W0141', 'label': [1451, 3971, 4037, 2733], 'feat': tensor([[10.0347,  9.8545,  6.5412,  ...,  9.3895,  8.6747,  7.7263],\n",
      "        [ 7.9894,  8.8298,  7.4322,  ...,  9.5085,  9.9919,  8.1064],\n",
      "        [10.3988, 10.4573,  8.6097,  ...,  9.6301,  9.7567,  8.3464],\n",
      "        ...,\n",
      "        [ 5.0362,  6.7749,  8.3047,  ...,  9.6442,  9.5334,  8.1903],\n",
      "        [ 8.7622,  9.1488,  8.7376,  ...,  8.6867,  9.2772,  8.3546],\n",
      "        [ 7.9294,  8.3848,  7.7726,  ...,  9.4468,  9.3716,  7.9354]])}\n",
      "{'key': 'BAC009S0044W0426', 'label': [384, 8, 4083, 2055, 2156, 2156, 3696, 477], 'feat': tensor([[4.9138, 5.3553, 6.2030,  ..., 9.3630, 8.9387, 6.8584],\n",
      "        [7.2723, 7.7249, 7.5132,  ..., 9.5916, 9.4239, 8.0279],\n",
      "        [7.1425, 2.4832, 8.1536,  ..., 9.8671, 9.4938, 8.0902],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])}\n",
      "{'key': 'BAC009S0599W0136', 'label': [1454, 387, 159, 1393, 2109, 3562], 'feat': tensor([[ 9.0226,  6.5339,  9.4066,  ...,  8.9375,  8.4903,  6.6428],\n",
      "        [11.0826, 10.9733,  9.0147,  ...,  8.9125,  9.1089,  6.9811],\n",
      "        [ 7.9070,  7.7264,  8.1196,  ...,  9.4669,  9.1850,  7.4919],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])}\n",
      "{'key': 'BAC009S0109W0299', 'label': [1242, 3658, 574, 3175, 2351, 1022, 2553, 355, 1778], 'feat': tensor([[ 8.8967,  8.7847,  6.5734,  ...,  9.5707,  8.7767,  7.5661],\n",
      "        [ 9.8950,  9.4926,  7.6122,  ...,  9.9590,  8.9419,  7.8388],\n",
      "        [10.8198, 10.2839,  7.7916,  ...,  9.6879,  9.2660,  7.8809],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])}\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataset):\n",
    "    print(data) \n",
    "    if i == 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69abf731-ac63-4944-8232-cd47ee48459a",
   "metadata": {},
   "source": [
    "### 数据batch化与padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "045feb6f-0081-4163-ad40-5f1e3d7037bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_conf = conf.get('batch_conf', {})\n",
    "dataset = Processor(dataset, processor.batch, **batch_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4923a35-f016-474a-a8f7-fd22ccb2abe7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'key': 'BAC009S0108W0199', 'label': [1656, 1621, 2807, 331, 506], 'feat': tensor([[12.2109, 11.8598,  7.3374,  ...,  9.4579,  9.4605,  7.6765],\n",
      "        [11.4925, 10.6988,  7.6509,  ...,  9.6277,  9.5402,  8.7528],\n",
      "        [ 9.4170,  8.3072,  7.3509,  ...,  9.6820,  9.4914,  8.6989],\n",
      "        ...,\n",
      "        [10.5787, 10.3363,  8.3428,  ...,  9.7901,  9.4125,  8.1679],\n",
      "        [10.4588, 10.0394,  7.9148,  ...,  9.8501,  9.1444,  7.2262],\n",
      "        [10.5515, 10.2172,  8.3560,  ...,  9.7540,  9.2594,  7.6902]])}, {'key': 'BAC009S0172W0128', 'label': [3608, 1143, 1409, 814], 'feat': tensor([[ 7.7422,  8.8199,  8.5580,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 7.8527,  7.5003,  6.7255,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 9.8539, 10.4513,  9.4371,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])}]\n",
      "[{'key': 'BAC009S0135W0398', 'label': [2, 2551, 2832, 2553, 1969, 3580, 1115, 2896, 3696, 477, 66], 'feat': tensor([[ 8.6287,  8.4061,  7.1290,  ...,  8.3146,  9.0892,  7.8816],\n",
      "        [10.1466, 10.6130,  9.9049,  ...,  9.6753,  8.7955,  7.4822],\n",
      "        [ 9.3461,  9.7615,  7.6914,  ...,  9.3870,  9.0112,  7.5605],\n",
      "        ...,\n",
      "        [ 9.6208,  9.2761,  4.1466,  ...,  9.4370,  9.5964,  7.4672],\n",
      "        [ 9.8597,  9.6723,  7.4107,  ..., 10.0448,  8.6580,  7.0420],\n",
      "        [10.2383,  9.4419,  8.9847,  ...,  9.0907,  8.7829,  6.5422]])}, {'key': 'BAC009S0599W0136', 'label': [1454, 387, 159, 1393, 2109, 3562], 'feat': tensor([[ 9.0227,  6.5274,  9.4065,  ...,  8.9393,  8.5107,  6.6695],\n",
      "        [11.0825, 10.9730,  9.0134,  ...,  8.8377,  9.0941,  7.0163],\n",
      "        [ 7.9080,  7.7279,  8.1212,  ...,  9.4147,  9.2111,  7.4019],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])}]\n",
      "[{'key': 'BAC009S0130W0390', 'label': [718, 3920, 3691, 426, 28, 1741, 2553, 1674, 3930, 3827], 'feat': tensor([[8.3960, 5.0236, 7.4287,  ..., 7.3534, 7.9297, 6.1057],\n",
      "        [6.8744, 7.3034, 7.7077,  ..., 8.5973, 8.3324, 6.4476],\n",
      "        [8.0931, 8.9837, 8.8009,  ..., 8.5568, 7.7565, 5.9887],\n",
      "        ...,\n",
      "        [6.8544, 7.5133, 7.4112,  ..., 7.8262, 7.2615, 4.9800],\n",
      "        [8.9602, 9.1029, 8.4344,  ..., 7.9275, 8.1456, 5.8418],\n",
      "        [8.1017, 7.9474, 8.0184,  ..., 7.1967, 7.0941, 6.5844]])}, {'key': 'BAC009S0004W0140', 'label': [1280, 124, 230, 36, 275, 2551, 275, 426, 273, 96, 257], 'feat': tensor([[6.5100, 6.4454, 6.0467,  ..., 8.6277, 7.4984, 6.2349],\n",
      "        [6.5339, 6.1121, 4.3315,  ..., 7.9804, 7.6516, 6.5690],\n",
      "        [5.6844, 6.1726, 2.9272,  ..., 7.9828, 8.0059, 6.1740],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])}]\n",
      "[{'key': 'BAC009S0026W0190', 'label': [25, 2993, 1164, 76, 1991, 10, 2358, 2035, 1991], 'feat': tensor([[10.5743,  9.3595,  8.7205,  ...,  9.5650,  9.1536,  8.1868],\n",
      "        [10.3056, 10.1966,  8.9973,  ...,  9.8401,  9.1781,  7.5213],\n",
      "        [10.2919, 10.0692,  8.2807,  ...,  9.4761,  9.4357,  7.4434],\n",
      "        ...,\n",
      "        [10.1842, 10.1591,  8.4725,  ..., 10.3844,  9.4613,  7.7987],\n",
      "        [ 8.7435,  8.6191,  7.7243,  ..., 10.7632, 10.3364,  7.9338],\n",
      "        [ 8.5207,  9.7174,  9.2990,  ..., 10.2764, 10.0418,  9.0315]])}, {'key': 'BAC009S0189W0141', 'label': [1451, 3971, 4037, 2733], 'feat': tensor([[ 9.9772,  9.7484,  6.7449,  ...,  3.1934,  3.6171,  4.5101],\n",
      "        [ 7.2121,  8.0175,  7.4661,  ...,  3.8012,  3.3823,  3.7318],\n",
      "        [10.2878, 10.3335,  8.1977,  ...,  4.0410,  3.6282,  3.9661],\n",
      "        ...,\n",
      "        [ 6.5680,  6.9037,  7.4214,  ...,  3.8853,  3.4261,  4.1828],\n",
      "        [ 8.3295,  9.1848,  9.2983,  ...,  3.4948,  2.7634,  3.6301],\n",
      "        [ 8.2454,  6.6432,  6.7549,  ...,  3.8419,  3.2740,  3.7515]])}]\n",
      "[{'key': 'BAC009S0074W0289', 'label': [769, 71, 3703, 1830, 2553, 1121, 724, 3880, 3830], 'feat': tensor([[8.1642, 8.3660, 7.6697,  ..., 8.8411, 9.5131, 8.1691],\n",
      "        [6.1234, 7.2340, 6.2039,  ..., 9.2325, 8.8204, 7.5751],\n",
      "        [9.6498, 9.5369, 6.8846,  ..., 9.8034, 8.9357, 8.1970],\n",
      "        ...,\n",
      "        [8.0584, 7.3205, 8.6633,  ..., 9.2776, 8.7942, 8.3519],\n",
      "        [9.0497, 9.0700, 5.0365,  ..., 8.9969, 9.1548, 7.5981],\n",
      "        [6.3995, 7.8235, 7.7921,  ..., 9.9845, 9.6448, 7.3447]])}, {'key': 'BAC009S0087W0124', 'label': [11, 477, 1149, 523, 1741, 2580, 1969], 'feat': tensor([[ 9.2192,  7.2671,  7.9409,  ..., 10.5305, 10.5506,  7.9892],\n",
      "        [11.3339, 11.4949,  9.2035,  ...,  9.7654,  9.2621,  7.0749],\n",
      "        [12.5999, 12.6130,  9.7985,  ..., 11.0373, 11.2443,  8.7662],\n",
      "        ...,\n",
      "        [ 7.1323,  5.7036,  8.4121,  ...,  9.5736,  9.0818,  7.3493],\n",
      "        [ 9.2875,  9.6687,  8.4556,  ...,  9.6356, 10.3619,  8.5063],\n",
      "        [ 7.6316,  7.1365,  4.9826,  ...,  8.9396,  9.1525,  7.8664]])}]\n",
      "[{'key': 'BAC009S0246W0415', 'label': [3517, 3517, 1176, 2318, 2553, 1107, 167, 95, 560], 'feat': tensor([[8.7242, 9.3763, 9.3614,  ..., 9.5309, 9.4385, 7.9913],\n",
      "        [7.9248, 8.1332, 5.7278,  ..., 9.8060, 9.5136, 8.1049],\n",
      "        [8.5409, 9.5663, 9.3832,  ..., 9.5949, 8.9061, 7.6636],\n",
      "        ...,\n",
      "        [7.4998, 7.3673, 6.6385,  ..., 9.5328, 9.3131, 7.5758],\n",
      "        [7.7944, 8.4934, 7.2805,  ..., 9.7965, 9.4260, 7.0329],\n",
      "        [7.5611, 8.3587, 8.6900,  ..., 9.7884, 9.1457, 7.5318]])}, {'key': 'BAC009S0152W0156', 'label': [3502, 1812, 1011, 3531, 1694, 814, 949, 1970, 19, 2476], 'feat': tensor([[ 6.6887,  7.8038,  0.0000,  ...,  9.6997,  9.5078,  7.7306],\n",
      "        [ 6.2701,  5.8444,  0.0000,  ...,  9.8808, 10.8600,  8.5129],\n",
      "        [ 8.6399,  9.2983,  0.0000,  ...,  9.2540,  9.8663,  7.8412],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])}]\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataset):\n",
    "    print(data) \n",
    "    if i == 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41dd0f90-796a-43cc-93ea-ae8cacd85200",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'key': 'BAC009S0246W0415',\n",
       "  'label': [3517, 3517, 1176, 2318, 2553, 1107, 167, 95, 560],\n",
       "  'feat': tensor([[8.7242, 9.3763, 9.3614,  ..., 9.5309, 9.4385, 7.9913],\n",
       "          [7.9248, 8.1332, 5.7278,  ..., 9.8060, 9.5136, 8.1049],\n",
       "          [8.5409, 9.5663, 9.3832,  ..., 9.5949, 8.9061, 7.6636],\n",
       "          ...,\n",
       "          [7.4998, 7.3673, 6.6385,  ..., 9.5328, 9.3131, 7.5758],\n",
       "          [7.7944, 8.4934, 7.2805,  ..., 9.7965, 9.4260, 7.0329],\n",
       "          [7.5611, 8.3587, 8.6900,  ..., 9.7884, 9.1457, 7.5318]])},\n",
       " {'key': 'BAC009S0152W0156',\n",
       "  'label': [3502, 1812, 1011, 3531, 1694, 814, 949, 1970, 19, 2476],\n",
       "  'feat': tensor([[ 6.6887,  7.8038,  0.0000,  ...,  9.6997,  9.5078,  7.7306],\n",
       "          [ 6.2701,  5.8444,  0.0000,  ...,  9.8808, 10.8600,  8.5129],\n",
       "          [ 8.6399,  9.2983,  0.0000,  ...,  9.2540,  9.8663,  7.8412],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df72f389-4b3e-498e-8fa8-ffb554bce716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feat_length = torch.tensor([x[\"feat\"].size(0) for x in data],dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "968d198e-8469-4e46-aefb-f8491f83d2bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(feat_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63e46b4d-7dbc-44a9-9c98-6f22f64d44ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照排序结果 返回其对应的索引结果\n",
    "order = torch.argsort(feat_length, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03db00e0-c6c9-4083-befa-646734f7cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Processor(dataset, processor.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "119a257a-aa15-4d59-91a5-0c2c4c04f50e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['BAC009S0128W0267', 'BAC009S0600W0140'], tensor([[[ 8.4586,  8.4750,  5.4352,  ...,  7.2068,  7.6271,  6.4186],\n",
      "         [10.1365, 10.4012,  8.7876,  ...,  8.2121,  7.3493,  6.0795],\n",
      "         [ 9.5767,  9.3461,  3.6846,  ...,  8.4466,  7.1648,  6.5419],\n",
      "         ...,\n",
      "         [ 8.3635,  8.3586,  6.5479,  ...,  8.9378,  8.3924,  6.7225],\n",
      "         [ 9.3292,  9.6250,  8.1633,  ...,  8.6924,  8.3276,  7.1249],\n",
      "         [ 7.4935,  6.0364,  6.7090,  ...,  8.0413,  8.8462,  7.5329]],\n",
      "\n",
      "        [[ 9.9886,  9.4007,  7.4236,  ...,  8.6365,  9.1940,  7.9169],\n",
      "         [10.1148,  9.8261,  8.4347,  ...,  9.3652,  9.6718,  8.4966],\n",
      "         [ 9.4363,  9.2812,  7.2438,  ...,  9.8238,  9.6255,  7.9771],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]), tensor([[  25,  427,   75, 2551,   70,  426,   56],\n",
      "        [1740, 4156, 2123, 1143,   -1,   -1,   -1]]), tensor([167, 163], dtype=torch.int32), tensor([7, 4], dtype=torch.int32))\n",
      "(['BAC009S0191W0123', 'BAC009S0035W0154'], tensor([[[11.3344, 11.0072,  8.2289,  ...,  4.5208,  2.8537,  3.7995],\n",
      "         [ 9.2672,  8.4630,  5.2720,  ...,  4.1959,  3.9098,  2.9402],\n",
      "         [ 9.6196,  8.9033,  7.3383,  ...,  3.9052,  3.3054,  3.4505],\n",
      "         ...,\n",
      "         [ 8.4137,  9.4562,  9.7045,  ...,  4.0985,  4.0763,  3.2577],\n",
      "         [ 8.7969,  8.5159,  3.4444,  ...,  4.2029,  4.0285,  4.1402],\n",
      "         [10.7465, 10.9569,  9.1476,  ...,  4.5674,  3.3605,  3.6163]],\n",
      "\n",
      "        [[10.1152, 10.1472,  8.3812,  ...,  9.9868,  9.5829,  8.3244],\n",
      "         [ 6.6497,  7.2472,  6.5855,  ...,  9.4542,  9.8203,  8.8255],\n",
      "         [ 9.0813,  9.6443,  8.3835,  ...,  9.7106,  9.8430,  9.1889],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]), tensor([[1991, 1016, 4037, 2733],\n",
      "        [ 702,   28, 2576,  366]]), tensor([201, 184], dtype=torch.int32), tensor([4, 4], dtype=torch.int32))\n",
      "(['BAC009S0127W0405', 'BAC009S0189W0141'], tensor([[[ 5.7501,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 8.4988,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 6.0121,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 6.9619,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 7.4833,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 6.3116,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[10.0351,  9.8551,  6.5406,  ...,  9.3288,  8.6887,  7.7803],\n",
      "         [ 7.9910,  8.8307,  7.4312,  ...,  9.3838,  9.9224,  8.0541],\n",
      "         [10.3985, 10.4568,  8.6079,  ...,  9.5893,  9.7547,  8.4077],\n",
      "         ...,\n",
      "         [ 7.9298,  8.3854,  7.7749,  ...,  9.4521,  9.4012,  7.9280],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]), tensor([[1018,  331, 2211, 3703,   28, 3828, 3414, 3428, 3142],\n",
      "        [1451, 3971, 4037, 2733,   -1,   -1,   -1,   -1,   -1]]), tensor([214, 212], dtype=torch.int32), tensor([9, 4], dtype=torch.int32))\n",
      "(['BAC009S0659W0324', 'BAC009S0235W0230'], tensor([[[ 8.6048,  9.0004,  8.4705,  ...,  0.0000, 11.8502,  9.9544],\n",
      "         [ 9.4799,  9.5853,  8.8789,  ...,  0.0000, 11.5166,  9.9734],\n",
      "         [ 9.3108,  9.2856,  7.8824,  ...,  0.0000, 11.8102,  9.9632],\n",
      "         ...,\n",
      "         [10.5396, 10.5960,  9.1145,  ...,  0.0000, 10.0490,  7.3239],\n",
      "         [10.5769,  9.5914,  7.8970,  ...,  0.0000, 10.2989,  8.2845],\n",
      "         [10.5737, 10.5799, 10.8513,  ...,  0.0000,  9.8205,  7.4677]],\n",
      "\n",
      "        [[ 5.2711,  7.0448,  7.6072,  ...,  9.6226,  8.9161,  6.3275],\n",
      "         [ 8.3455,  8.7281,  6.6039,  ...,  9.4482,  8.9093,  6.7663],\n",
      "         [ 7.4761,  8.2769,  7.7257,  ...,  9.7046,  9.4059,  7.7037],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]), tensor([[3703, 3796, 1694, 4011, 3414, 3429,  306, 2553, 3925, 4077],\n",
      "        [4011, 3414,  482, 1674, 3502, 1638,   -1,   -1,   -1,   -1]]), tensor([229, 215], dtype=torch.int32), tensor([10,  6], dtype=torch.int32))\n",
      "(['BAC009S0202W0406', 'BAC009S0511W0122'], tensor([[[ 8.5449,  8.3801,  6.9520,  ..., 11.1131, 10.1464,  8.1023],\n",
      "         [10.3899, 10.0974,  7.5315,  ..., 11.0948, 10.7895,  9.0102],\n",
      "         [10.3491,  9.0129,  7.1967,  ..., 10.5325, 10.7049,  8.9607],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 9.8029,  9.8499,  7.8711,  ...,  9.3414,  8.8141,  7.2558],\n",
      "         [ 8.9192,  9.3950,  8.1144,  ...,  9.1202,  8.5489,  6.8648],\n",
      "         [ 8.7123,  8.0683,  8.4694,  ...,  8.8759,  8.3838,  6.9664],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]), tensor([[2483,   71, 4057, 2576, 1400,  771,   -1],\n",
      "        [  10, 3140, 3560, 1932, 2397,    8,  428]]), tensor([238, 232], dtype=torch.int32), tensor([6, 7], dtype=torch.int32))\n",
      "(['BAC009S0044W0426', 'BAC009S0017W0251'], tensor([[[ 4.6670,  5.1192,  0.0000,  ...,  9.6385,  9.8300,  7.4352],\n",
      "         [ 6.8735,  6.8337,  0.0000,  ...,  9.8808,  9.3433,  8.5645],\n",
      "         [ 7.2836,  7.6655,  0.0000,  ...,  9.3444,  9.6414,  7.6837],\n",
      "         ...,\n",
      "         [ 8.3975,  8.1936,  0.0000,  ...,  9.3961,  9.8383,  8.8075],\n",
      "         [ 7.2510,  7.4284,  0.0000,  ...,  9.1622, 10.1518,  8.2457],\n",
      "         [ 3.8655,  5.4604,  0.0000,  ...,  9.2287, 10.0109,  8.8139]],\n",
      "\n",
      "        [[ 8.0368,  8.4134,  6.8614,  ...,  9.7831,  9.4139,  7.7385],\n",
      "         [ 8.0663,  8.7596,  7.9227,  ..., 10.0486,  9.5369,  7.3860],\n",
      "         [ 6.8677,  8.1551,  8.6109,  ...,  9.9032,  9.3496,  7.8278],\n",
      "         ...,\n",
      "         [ 8.4017,  9.0341,  8.2494,  ..., 11.1876, 10.8796,  9.3109],\n",
      "         [ 8.3789,  8.6269,  6.3244,  ..., 10.8945, 11.1904,  9.0844],\n",
      "         [ 9.2008,  9.9915,  9.4013,  ..., 10.5520, 10.2344,  9.0678]]]), tensor([[ 384,    8, 4083, 2055, 2156, 2156, 3696,  477],\n",
      "        [1544, 3704, 4004, 2005,  338, 2087, 1618, 3738]]), tensor([239, 239], dtype=torch.int32), tensor([8, 8], dtype=torch.int32))\n"
     ]
    }
   ],
   "source": [
    "for i, train_data in enumerate(dataset):\n",
    "    print(train_data) \n",
    "    if i == 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "259621cd-3c8a-411f-9ef2-ea078a65112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23cdbf85-b0ed-4b96-889b-199148995c42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = torch.ones(2, 4)\n",
    "b = torch.ones(4, 4)\n",
    "c = torch.ones(5, 4)\n",
    "pd_data = pad_sequence([a, b, c], batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffe5b279-364f-4b0b-98b3-e01ebf99bf03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e371a3-289a-4a0c-b4fb-24bf6fc68fab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "720e596b-3a16-4037-9d08-4d432fe32e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\n"
     ]
    }
   ],
   "source": [
    "from wenet.utils.init_model import init_model\n",
    "# from torch.utils import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3e18ea9-ea7b-4e23-a026-fe5c70bc5cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = r'exp/conformer/train.yaml'\n",
    "\n",
    "with open(path, 'r') as fin:\n",
    "    configs = yaml.load(fin, Loader=yaml.FullLoader)\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d7d439d-f3f3-45f6-bc2b-bdbd998757aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = init_model(configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02211d6-bc02-4831-8a17-6f9164300603",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef593f59-0288-4cef-aa24-b02905ece3ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'key': 'BAC009S0246W0415',\n",
       "  'label': [3517, 3517, 1176, 2318, 2553, 1107, 167, 95, 560],\n",
       "  'feat': tensor([[8.7242, 9.3763, 9.3614,  ..., 9.5309, 9.4385, 7.9913],\n",
       "          [7.9248, 8.1332, 5.7278,  ..., 9.8060, 9.5136, 8.1049],\n",
       "          [8.5409, 9.5663, 9.3832,  ..., 9.5949, 8.9061, 7.6636],\n",
       "          ...,\n",
       "          [7.4998, 7.3673, 6.6385,  ..., 9.5328, 9.3131, 7.5758],\n",
       "          [7.7944, 8.4934, 7.2805,  ..., 9.7965, 9.4260, 7.0329],\n",
       "          [7.5611, 8.3587, 8.6900,  ..., 9.7884, 9.1457, 7.5318]])},\n",
       " {'key': 'BAC009S0152W0156',\n",
       "  'label': [3502, 1812, 1011, 3531, 1694, 814, 949, 1970, 19, 2476],\n",
       "  'feat': tensor([[ 6.6887,  7.8038,  0.0000,  ...,  9.6997,  9.5078,  7.7306],\n",
       "          [ 6.2701,  5.8444,  0.0000,  ...,  9.8808, 10.8600,  8.5129],\n",
       "          [ 8.6399,  9.2983,  0.0000,  ...,  9.2540,  9.8663,  7.8412],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aaed02dc-9bd6-4b87-8142-6480bab8d360",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['BAC009S0044W0426', 'BAC009S0017W0251'],\n",
       " tensor([[[ 4.6670,  5.1192,  0.0000,  ...,  9.6385,  9.8300,  7.4352],\n",
       "          [ 6.8735,  6.8337,  0.0000,  ...,  9.8808,  9.3433,  8.5645],\n",
       "          [ 7.2836,  7.6655,  0.0000,  ...,  9.3444,  9.6414,  7.6837],\n",
       "          ...,\n",
       "          [ 8.3975,  8.1936,  0.0000,  ...,  9.3961,  9.8383,  8.8075],\n",
       "          [ 7.2510,  7.4284,  0.0000,  ...,  9.1622, 10.1518,  8.2457],\n",
       "          [ 3.8655,  5.4604,  0.0000,  ...,  9.2287, 10.0109,  8.8139]],\n",
       " \n",
       "         [[ 8.0368,  8.4134,  6.8614,  ...,  9.7831,  9.4139,  7.7385],\n",
       "          [ 8.0663,  8.7596,  7.9227,  ..., 10.0486,  9.5369,  7.3860],\n",
       "          [ 6.8677,  8.1551,  8.6109,  ...,  9.9032,  9.3496,  7.8278],\n",
       "          ...,\n",
       "          [ 8.4017,  9.0341,  8.2494,  ..., 11.1876, 10.8796,  9.3109],\n",
       "          [ 8.3789,  8.6269,  6.3244,  ..., 10.8945, 11.1904,  9.0844],\n",
       "          [ 9.2008,  9.9915,  9.4013,  ..., 10.5520, 10.2344,  9.0678]]]),\n",
       " tensor([[ 384,    8, 4083, 2055, 2156, 2156, 3696,  477],\n",
       "         [1544, 3704, 4004, 2005,  338, 2087, 1618, 3738]]),\n",
       " tensor([239, 239], dtype=torch.int32),\n",
       " tensor([8, 8], dtype=torch.int32))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33e9e36-1232-496f-8668-2c97bf4498f0",
   "metadata": {},
   "source": [
    "### encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6124a7e4-e8c7-4945-bf43-86eefa84b27f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "asr_encoder = model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ec5aa6a-a6af-431c-8c2c-4fb9884ae2e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-cfb0728fdbca>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  speech, text,  speech_length, text_length =  torch.tensor(train_data[1]), torch.tensor(train_data[2]),torch.tensor(train_data[3]),torch.tensor(train_data[4])\n"
     ]
    }
   ],
   "source": [
    "# 构建输入\n",
    "speech, text,  speech_length, text_length =  torch.tensor(train_data[1]), torch.tensor(train_data[2]),torch.tensor(train_data[3]),torch.tensor(train_data[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83bc4a7a-6e03-49b8-bf7c-77f16d47ca3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder_out, encoder_mask = asr_encoder(speech, speech_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f982c0b-9105-4c45-8ab1-a566c352e5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 239, 80])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0519b7e5-d086-40ad-bd2c-4b0f0d2ef924",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_length.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "17f89b2d-9a45-47d6-8a58-93a7cc0d1928",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 59, 256]), torch.Size([2, 1, 59]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练输出\n",
    "encoder_out.shape, encoder_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8208c41c-8fc3-451d-9e3d-bfe3de82b67a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True]],\n",
       "\n",
       "        [[True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "634dbbd5-df6d-4272-ad7c-738baa7f978b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encode_mask_lens\n",
    "encoder_out_l = encoder_mask.squeeze(1).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "250b1b3d-f59e-445c-9b3b-f4eea8e9e3aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([59, 59])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_out_l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6842b2a-8947-4ee9-983c-0b926ba547fb",
   "metadata": {},
   "source": [
    "### Decoder模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4032ac68-abe9-4abf-b83b-51edf5248c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from wenet.utils.common import (IGNORE_ID, add_sos_eos, log_add,\n",
    "                                remove_duplicates_and_blank, th_accuracy,\n",
    "                                reverse_pad_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97902de6-b8cd-4788-aff7-d9efff075bed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sos, eos = configs[\"output_dim\"] - 1, configs[\"output_dim\"] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "66ef37d5-7740-45a4-9740-50d776a233cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ys_in_pad, ys_out_pad = add_sos_eos(text, sos, eos, -1)\n",
    "ys_in_lens = text_length + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f42f1d17-457e-43a6-8105-f7ecdcbb491f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4232,  384,    8, 4083, 2055, 2156, 2156, 3696,  477],\n",
       "         [4232, 1544, 3704, 4004, 2005,  338, 2087, 1618, 3738]]),\n",
       " tensor([[ 384,    8, 4083, 2055, 2156, 2156, 3696,  477, 4232],\n",
       "         [1544, 3704, 4004, 2005,  338, 2087, 1618, 3738, 4232]]),\n",
       " torch.Size([2, 9]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys_in_pad, ys_out_pad, ys_out_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b4deeccf-37cf-4a5e-bada-17fae43d620a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reverse the seq, used for right to left decoder\n",
    "r_ys_pad = reverse_pad_list(text, text_length, float(-1.0))\n",
    "r_ys_in_pad, r_ys_out_pad = add_sos_eos(r_ys_pad, sos, eos, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5602dcaa-7d65-4891-a994-ac115955a3e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4232,  477, 3696, 2156, 2156, 2055, 4083,    8,  384],\n",
       "         [4232, 3738, 1618, 2087,  338, 2005, 4004, 3704, 1544]]),\n",
       " tensor([[ 477, 3696, 2156, 2156, 2055, 4083,    8,  384, 4232],\n",
       "         [3738, 1618, 2087,  338, 2005, 4004, 3704, 1544, 4232]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_ys_in_pad, r_ys_out_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1818e5b1-3c76-4445-bc6f-678fc561d749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "asr_decoder = model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3c109e40-ee1b-40a9-9838-c9c94afe640e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoder_out, r_decoder_out, _ = asr_decoder(encoder_out, encoder_mask,\n",
    "                                             ys_in_pad, ys_in_lens,\n",
    "                                             r_ys_in_pad,\n",
    "                                             reverse_weight=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d671d80d-7bd3-4f17-b29d-f95c098b3ced",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1381, -0.5130,  0.3466,  ..., -1.8519, -0.7287,  0.2848],\n",
       "          [ 0.6350, -0.6088, -0.4716,  ..., -0.4503, -0.2493, -0.0035],\n",
       "          [-0.2735,  0.8892, -0.2949,  ..., -0.4250, -0.1103,  0.0653],\n",
       "          ...,\n",
       "          [-0.1359,  0.1031, -0.2078,  ...,  0.3344,  0.5821,  0.8283],\n",
       "          [-0.1494, -0.2119,  0.0921,  ..., -0.8839,  0.1637, -0.6406],\n",
       "          [-0.0175, -0.4482,  0.5752,  ...,  0.0692, -0.5830,  0.1351]],\n",
       " \n",
       "         [[-0.2971, -0.4606,  0.0773,  ..., -1.7014, -0.5726, -0.2149],\n",
       "          [-0.8763, -0.8359, -0.2438,  ...,  0.4683,  0.3095,  0.2194],\n",
       "          [-0.1976,  0.1418,  0.1229,  ..., -0.2470, -0.5907,  1.0407],\n",
       "          ...,\n",
       "          [-0.1481,  0.0304, -0.3653,  ...,  0.3072, -0.1863, -0.4751],\n",
       "          [-0.9997, -0.0741,  0.6237,  ...,  0.7740,  0.6361, -0.1804],\n",
       "          [-0.3859,  0.7098, -0.1761,  ..., -1.0280,  1.0833, -0.5277]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " torch.Size([2, 9, 4233]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_out, decoder_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5269da21-9e84-4e82-9531-2069c21bd3bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_decoder_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb97c214-69b6-4582-801f-dc716796196b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 计算decode损失\n",
    "[标签平滑Label Smoothing](https://blog.csdn.net/AZ_CHEN/article/details/127658050)  \n",
    "[机器学习：Kullback-Leibler Divergence （KL 散度）](https://blog.csdn.net/matrix_space/article/details/80550561)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "86d6b205-5866-4f12-908c-4b6269b058d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 384,    8, 4083, 2055, 2156, 2156, 3696,  477, 4232],\n",
       "        [1544, 3704, 4004, 2005,  338, 2087, 1618, 3738, 4232]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys_out_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "77130026-5b0f-4bcf-aa38-e4c9ad828b10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9, 4233])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e2f97c9-5896-4246-9810-677295d1d4e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.6013,  0.0455, -0.5443,  1.8900],\n",
       "        [-0.3198, -0.8287, -0.5038,  0.0000,  0.7186],\n",
       "        [ 1.4001,  1.2908,  0.8427, -1.9264,  0.0000]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label smoothing \n",
    "torch.randn([3,5]).scatter_(1, torch.tensor([[0]\n",
    "                                             ,[3]\n",
    "                                             , [4]])\n",
    "                            , 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b644d22e-fe85-4168-a9d3-ee71e5ddb32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c8c6c-f78e-411b-8b30-4bf748d06e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dfdc79-b502-42a5-952c-80d6668e2ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2a58633-173d-4279-a3e6-4d079c763ed9",
   "metadata": {},
   "source": [
    "## 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "172ebd62-6295-4fcf-9504-c74df051c19a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c125ce-a304-4594-bbc7-35294799562a",
   "metadata": {},
   "source": [
    "### attention_beam_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab6084-af68-4f92-a1b9-c614ddf35ba5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Encoder输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "3e64deb2-756d-4065-b2b6-ffbf19f3dab1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config\n",
    "batch_size = 2\n",
    "beam_size = 3\n",
    "\n",
    "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "0569cc2f-9073-4602-9de2-c2c1003bdc4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 基本配置\n",
    "maxlen = encoder_out.size(1)\n",
    "encoder_dim = encoder_out.size(2)\n",
    "running_size = batch_size * beam_size\n",
    "running_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "5b209ed1-07cb-4d9a-b367-c351e451e671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 59, 256]), torch.Size([2, 1, 59]))"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_out, encoder_mask = asr_encoder(speech, speech_length)\n",
    "encoder_out.shape, encoder_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "f17c5809-adc4-4fe0-b20e-7a7f85575ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 59, 256])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B*N, T, H\n",
    "encoder_out = encoder_out.unsqueeze(1).repeat(1, beam_size, 1, 1).view(running_size, maxlen, encoder_dim)  # (B*N, maxlen, encoder_dim)\n",
    "encoder_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "0a822e84-3f12-4332-9ebf-284c78685e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7055, 0.2328, 0.0598],\n",
      "         [0.0949, 0.3555, 0.8259]],\n",
      "\n",
      "        [[0.8999, 0.8942, 0.7527],\n",
      "         [0.4604, 0.4847, 0.0536]]]) \n",
      " tensor([[[[0.7055, 0.2328, 0.0598],\n",
      "          [0.0949, 0.3555, 0.8259]]],\n",
      "\n",
      "\n",
      "        [[[0.8999, 0.8942, 0.7527],\n",
      "          [0.4604, 0.4847, 0.0536]]]]) \n",
      " tensor([[[[0.7055, 0.2328, 0.0598],\n",
      "          [0.0949, 0.3555, 0.8259]],\n",
      "\n",
      "         [[0.7055, 0.2328, 0.0598],\n",
      "          [0.0949, 0.3555, 0.8259]],\n",
      "\n",
      "         [[0.7055, 0.2328, 0.0598],\n",
      "          [0.0949, 0.3555, 0.8259]]],\n",
      "\n",
      "\n",
      "        [[[0.8999, 0.8942, 0.7527],\n",
      "          [0.4604, 0.4847, 0.0536]],\n",
      "\n",
      "         [[0.8999, 0.8942, 0.7527],\n",
      "          [0.4604, 0.4847, 0.0536]],\n",
      "\n",
      "         [[0.8999, 0.8942, 0.7527],\n",
      "          [0.4604, 0.4847, 0.0536]]]])\n"
     ]
    }
   ],
   "source": [
    "# eg\n",
    "a = torch.rand(2, 2, 3)\n",
    "b = a.unsqueeze(1)\n",
    "c = b.repeat(1, 3, 1, 1)\n",
    "print(a, \"\\n\", b,\"\\n\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "e2a2be40-46d5-4d91-b33f-5a8622588c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_mask = encoder_mask.unsqueeze(1).repeat(\n",
    "    1, beam_size, 1, 1).view(running_size, 1,\n",
    "                             maxlen)  # (B*N, 1, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "afea254e-59a9-4f8f-b475-e62b1caac431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 59])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "fbf28c49-13a3-468d-8c5d-d5f9adc3167f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1],\n",
       "        [-1],\n",
       "        [-1],\n",
       "        [-1],\n",
       "        [-1],\n",
       "        [-1]], device='cuda:0')"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyps = torch.ones([running_size, 1], dtype=torch.long,\n",
    "                  device=device).fill_(-1)  # (B*N, 1)\n",
    "hyps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "44a69b1f-b061-4717-b742-57c0c15e2d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., -inf, -inf])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.tensor([0.0] + [-float('inf')] * (beam_size - 1),\n",
    "                      dtype=torch.float)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "9f75c6be-1e1f-4042-b3e8-408c4a44657d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [-inf],\n",
       "        [-inf],\n",
       "        [0.],\n",
       "        [-inf],\n",
       "        [-inf]], device='cuda:0')"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = scores.to(device).repeat([batch_size]).unsqueeze(1).to(\n",
    "    device)  # (B*N, 1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "99ab0b93-4d9b-4373-8971-0e5d27e3419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end_flag: [B*N, 1 ]\n",
    "end_flag = torch.zeros_like(scores, dtype=torch.bool, device=device)\n",
    "cache: Optional[List[torch.Tensor]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "e29ec2c7-54c9-464a-93a5-e5869b32dc5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False]], device='cuda:0')"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "b9ade88e-7bb0-4836-89c9-dffcd8f5e7d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1],\n",
       "        [-1],\n",
       "        [-1],\n",
       "        [-1],\n",
       "        [-1],\n",
       "        [-1]], device='cuda:0')"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "f294c500-75ad-4882-a33f-496af8dd2754",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [-inf],\n",
       "        [-inf],\n",
       "        [0.],\n",
       "        [-inf],\n",
       "        [-inf]], device='cuda:0')"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6456f3-8d37-48f3-9748-f511fc953998",
   "metadata": {},
   "source": [
    "#### decoder解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "74b41618-c35c-4f50-81a2-8abdcd81b61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_finished_scores(score: torch.Tensor,\n",
    "                         flag: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    If a sequence is finished, we only allow one alive branch. This function\n",
    "    aims to give one branch a zero score and the rest -inf score.\n",
    "\n",
    "    Args:\n",
    "        score (torch.Tensor): A real value array with shape\n",
    "            (batch_size * beam_size, beam_size).\n",
    "        flag (torch.Tensor): A bool array with shape\n",
    "            (batch_size * beam_size, 1).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: (batch_size * beam_size, beam_size).\n",
    "    \"\"\"\n",
    "    beam_size = score.size(-1)\n",
    "    zero_mask = torch.zeros_like(flag, dtype=torch.bool)\n",
    "    if beam_size > 1:\n",
    "        unfinished = torch.cat((zero_mask, flag.repeat([1, beam_size - 1])),\n",
    "                               dim=1)\n",
    "        finished = torch.cat((flag, zero_mask.repeat([1, beam_size - 1])),\n",
    "                             dim=1)\n",
    "    else:\n",
    "        unfinished = zero_mask\n",
    "        finished = flag\n",
    "    print(unfinished, \"\\n\", finished)\n",
    "    score.masked_fill_(unfinished, -float('inf'))\n",
    "    score.masked_fill_(finished, 0)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "d9c699d6-36a1-4193-b447-340cc0b20377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mask_finished_preds(pred: torch.Tensor, flag: torch.Tensor,\n",
    "                        eos: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    If a sequence is finished, all of its branch should be <eos>\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): A int array with shape\n",
    "            (batch_size * beam_size, beam_size).\n",
    "        flag (torch.Tensor): A bool array with shape\n",
    "            (batch_size * beam_size, 1).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: (batch_size * beam_size).\n",
    "    \"\"\"\n",
    "    beam_size = pred.size(-1)\n",
    "    finished = flag.repeat([1, beam_size])\n",
    "    return pred.masked_fill_(finished, eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "62cef0c6-fc47-4797-a47c-18ae7616bf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(batch_size*beam_size, 59 ,10).to(device)\n",
    "y = x[:, -1]\n",
    "log_b = torch.log_softmax(y, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "8957256d-b537-431d-ae9f-ec183aee366d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5, 7, 8],\n",
       "         [8, 7, 0],\n",
       "         [3, 0, 5],\n",
       "         [3, 4, 0],\n",
       "         [1, 3, 4],\n",
       "         [7, 8, 2]], device='cuda:0'),\n",
       " tensor([[-1.8687, -1.9017, -1.9936],\n",
       "         [-2.0534, -2.0807, -2.0844],\n",
       "         [-1.9574, -1.9693, -2.0190],\n",
       "         [-1.8002, -1.9011, -1.9621],\n",
       "         [-2.0147, -2.0761, -2.1046],\n",
       "         [-2.0436, -2.0444, -2.0675]], device='cuda:0'))"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_logp, top_k_index = log_b.topk(beam_size)\n",
    "top_k_index, top_k_logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "b643b0d7-b761-43f5-8e93-df608ec4d4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False, False]], device='cuda:0') \n",
      " tensor([[False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False, False]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "top_k_logp = mask_finished_scores(top_k_logp, end_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "03d04ea7-00cd-4f47-809f-55de70a4205e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_k_index = mask_finished_preds(top_k_index, end_flag, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "347623ef-2086-41b0-984d-74e5427bf930",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.7171, -5.7501, -5.8420],\n",
       "        [-5.9236, -5.9509, -5.9546],\n",
       "        [-5.9298, -5.9418, -5.9915],\n",
       "        [-5.6771, -5.7780, -5.8390],\n",
       "        [-5.9491, -6.0104, -6.0389],\n",
       "        [-5.9801, -5.9809, -6.0040]], device='cuda:0')"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选择top-k的数据进行存储\n",
    "scores = scores + top_k_logp\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "936d4d6f-b35e-4fa0-81a4-159734f8bac7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.7171, -5.7501, -5.8420, -5.9236, -5.9509, -5.9546, -5.9298, -5.9418,\n",
       "         -5.9915],\n",
       "        [-5.6771, -5.7780, -5.8390, -5.9491, -6.0104, -6.0389, -5.9801, -5.9809,\n",
       "         -6.0040]], device='cuda:0')"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = scores.view(batch_size, beam_size*beam_size)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "2b26c851-3955-4a19-8761-ad108bf31381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores, offset_k_index = scores.topk(k=beam_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "2b184e6a-dcac-45a6-9318-312f19f44f52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-5.7171, -5.7501, -5.8420],\n",
       "         [-5.6771, -5.7780, -5.8390]], device='cuda:0'),\n",
       " tensor([[0, 1, 2],\n",
       "         [0, 1, 2]], device='cuda:0'))"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores, offset_k_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "5c2aab55-0b6f-47fc-9495-2ed0cdbb3939",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_index = (offset_k_index // beam_size).view(-1)\n",
    "cache_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "364963f2-b3a7-4295-9ac4-e780eb7944a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_cache_index = (torch.arange(batch_size, device=device).view(\n",
    "                -1, 1).repeat([1, beam_size]) * beam_size).view(-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "c64faa8f-353d-4b0e-9118-6a70861af962",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 3, 3, 3], device='cuda:0')"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_cache_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "14b2530d-1914-4115-8702-fad0b0ea0ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 3, 3, 3], device='cuda:0')"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_index = base_cache_index + cache_index\n",
    "cache_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "4791129b-aec1-4427-9fb2-7c8d9f79285b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.7171],\n",
       "        [-5.7501],\n",
       "        [-5.8420],\n",
       "        [-5.6771],\n",
       "        [-5.7780],\n",
       "        [-5.8390]], device='cuda:0')"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = scores.view(-1, 1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "53a51aa9-327d-455d-953e-e96ea303422e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [9, 9, 9]], device='cuda:0')"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_k_index = torch.arange(batch_size, device=device).view(\n",
    "    -1, 1).repeat([1, beam_size])  # (B, N)\n",
    "base_k_index = base_k_index * beam_size * beam_size\n",
    "base_k_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "473f216e-2df4-40ab-9585-e260c302ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k_index = base_k_index.view(-1) + offset_k_index.view(\n",
    "                -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "8b2ea085-fc0c-4e1a-8164-5c3cc540f1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.7171],\n",
       "        [-5.7501],\n",
       "        [-5.8420],\n",
       "        [-5.6771],\n",
       "        [-5.7780],\n",
       "        [-5.8390]], device='cuda:0')"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "52de4764-32f3-4f73-a0c0-e79e87cc2f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  9, 10, 11], device='cuda:0')"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_k_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "e07501c2-0094-4091-9d3b-72e15e1f4544",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  1,  2,  9, 10, 11], device='cuda:0'),\n",
       " tensor([[5, 7, 8],\n",
       "         [8, 7, 0],\n",
       "         [3, 0, 5],\n",
       "         [3, 4, 0],\n",
       "         [1, 3, 4],\n",
       "         [7, 8, 2]], device='cuda:0'))"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_k_index, top_k_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "081e32df-2123-4596-8e35-f3be35b19fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 7, 8, 3, 4, 0], device='cuda:0')"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_k_pred = torch.index_select(top_k_index.view(-1), dim=-1,index=best_k_index)\n",
    "best_k_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "e4dfebff-3c04-4987-93aa-ef9b6d273b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 3, 3, 3], device='cuda:0')"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyps_index = best_k_index // beam_size\n",
    "best_hyps_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "0057c4d5-1446-461c-b800-7df0257d4818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1,  7,  5],\n",
       "        [-1,  7,  5],\n",
       "        [-1,  7,  5],\n",
       "        [-1,  0,  7],\n",
       "        [-1,  0,  7],\n",
       "        [-1,  0,  7]], device='cuda:0')"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_best_k_hyps = torch.index_select(\n",
    "                hyps, dim=0, index=best_hyps_index)  \n",
    "last_best_k_hyps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "29e56e22-dccc-42a4-9814-945da50c557b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1,  7,  5,  5],\n",
       "        [-1,  7,  5,  7],\n",
       "        [-1,  7,  5,  8],\n",
       "        [-1,  0,  7,  3],\n",
       "        [-1,  0,  7,  4],\n",
       "        [-1,  0,  7,  0]], device='cuda:0')"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyps = torch.cat((last_best_k_hyps, best_k_pred.view(-1, 1)),\n",
    "                             dim=1) \n",
    "hyps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "b5084afb-3d38-4c18-918c-0e06dd06e9a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False]], device='cuda:0')"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_flag = torch.eq(hyps[:, -1], 1000).view(-1, 1)\n",
    "end_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "9cf13a9a-3f7a-43d5-a4ed-c5b0e4ebac25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.7171],\n",
       "        [-5.7501],\n",
       "        [-5.8420],\n",
       "        [-5.6771],\n",
       "        [-5.7780],\n",
       "        [-5.8390]], device='cuda:0')"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "d2887399-6f5b-41fc-9884-e77067e4a620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## get_所有可能的序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "8c50e67a-53ed-4496-8ae9-0b732b837409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = scores.view(batch_size, beam_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "2907b8e8-1057-4ab3-a088-7f777f584bea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-5.7171, -5.6771], device='cuda:0'), tensor([0, 0], device='cuda:0'))"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score, best_index = scores.max(dim=-1)\n",
    "best_score, best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "095c9ba9-3f03-4aea-8a84-47146633061b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3], device='cuda:0')"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyps_index = best_index + torch.arange(\n",
    "            batch_size, dtype=torch.long, device=device) * beam_size\n",
    "best_hyps_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "80fdc684-9868-4c98-9456-bf55c21df46d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1,  1,  9,  5],\n",
       "        [-1,  9,  3,  7]], device='cuda:0')"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyps = torch.index_select(hyps, dim=0, index=best_hyps_index)\n",
    "best_hyps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4d42f1-fd38-45e2-bafc-6fb736642b14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
