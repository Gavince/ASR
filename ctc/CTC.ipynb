{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC（ Connectionist Temporal Classification，连接时序分类）是一种用于序列建模的工具，其核心是定义了特殊的**目标函数/优化准则**[1]。\n",
    "\n",
    "> jupyter notebook 版见 [repo](https://github.com/DingKe/ml-tutorial/tree/master/ctc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. 算法\n",
    "这里大体根据 Alex Grave 的开山之作[1]，讨论 CTC 的算法原理，并基于 numpy 从零实现 CTC 的推理及训练算法。\n",
    "\n",
    "## 1.1 序列问题形式化。\n",
    "序列问题可以形式化为如下函数：\n",
    "\n",
    "$$N_w: (R^m)^T \\rightarrow (R^n)^T$$\n",
    "其中，序列目标为字符串（词表大小为 $n$），即 $N_w$ 输出为 $n$ 维多项概率分布（e.g. 经过 softmax 处理）。\n",
    "\n",
    "网络输出为：$y = N_w(x)$，其中，$y_k^t$ $t$ 表示时刻第 $k$ 项的概率。\n",
    "\n",
    "![](https://distill.pub/2017/ctc/assets/full_collapse_from_audio.svg)\n",
    "**图1. 序列建模【[src](https://distill.pub/2017/ctc/)】**\n",
    "\n",
    "\n",
    "虽然并没为限定 $N_w$ 具体形式，下面为假设其了某种神经网络（e.g. RNN）。\n",
    "下面代码示例 toy $N_w$："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24654511 0.18837589 0.16937668 0.16757465 0.22812766]\n",
      " [0.25443629 0.14992236 0.22945293 0.17240658 0.19378184]\n",
      " [0.24134404 0.17179604 0.23572466 0.12994237 0.22119288]\n",
      " [0.27216255 0.13054313 0.2679252  0.14184499 0.18752413]\n",
      " [0.32558002 0.13485564 0.25228604 0.09743785 0.18984045]\n",
      " [0.23855586 0.14800386 0.23100255 0.17158135 0.21085638]\n",
      " [0.38534786 0.11524603 0.18220093 0.14617864 0.17102655]\n",
      " [0.21867406 0.18511892 0.21305488 0.16472572 0.21842642]\n",
      " [0.29856607 0.13646801 0.27196606 0.11562552 0.17737434]\n",
      " [0.242347   0.14102063 0.21716951 0.2355229  0.16393996]\n",
      " [0.26597326 0.10009752 0.23362892 0.24560198 0.15469832]\n",
      " [0.23337289 0.11918746 0.28540761 0.20197928 0.16005275]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1111)\n",
    "\n",
    "T, V = 12, 5\n",
    "m, n = 6, V\n",
    "\n",
    "x = np.random.random([T, m])  # T x m\n",
    "w = np.random.random([m, n])  # weights, m x n\n",
    "\n",
    "\n",
    "def softmax(logits):\n",
    "    max_value = np.max(logits, axis=1, keepdims=True)\n",
    "    exp = np.exp(logits - max_value)\n",
    "    exp_sum = np.sum(exp, axis=1, keepdims=True)\n",
    "    dist = exp / exp_sum\n",
    "    return dist\n",
    "\n",
    "\n",
    "def toy_nw(x):\n",
    "    y = np.matmul(x, w)  # T x n\n",
    "    y = softmax(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "y = toy_nw(x)\n",
    "print(y)\n",
    "print(y.sum(1, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 align-free 变长映射\n",
    "上面的形式是输入和输出的一对一的映射。序列学习任务一般而言是多对多的映射关系（如语音识别中，上百帧输出可能仅对应若干音节或字符，并且每个输入和输出之间，也没有清楚的对应关系）。CTC 通过引入一个特殊的 blank 字符（用 % 表示），解决多对一映射问题。\n",
    "\n",
    "扩展原始词表 $L$ 为 $L^\\prime = L \\cup \\{\\text{blank}\\}$。对输出字符串，定义操作 $B$：1）合并连续的相同符号；2）去掉 blank 字符。\n",
    "\n",
    "例如，对于 “aa%bb%%cc”，应用 $B$，则实际上代表的是字符串 \"abc\"。同理“%a%b%cc%” 也同样代表 \"abc\"。\n",
    "$$\n",
    "B(aa\\%bb\\%\\%cc) = B(\\%a\\%b\\%cc\\%) = abc\n",
    "$$\n",
    "\n",
    "通过引入blank 及 $B$，可以实现了变长的映射。\n",
    "$$\n",
    "L^{\\prime T} \\rightarrow L^{\\le T}\n",
    "$$\n",
    "\n",
    "\t因为这个原因，CTC 只能建模输出长度小于输入长度的序列问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 似然计算\n",
    "和大多数有监督学习一样，CTC 使用最大似然标准进行训练。\n",
    "\n",
    "给定输入 $x$，输出 $l$ 的条件概率为：\n",
    "$$\n",
    "p(l|x) =  \\sum_{\\pi \\in B^{-1}(l)} p(\\pi|x)\n",
    "$$\n",
    "\n",
    "其中，$B^{-1}(l)$ 表示了长度为 $T$ 且示经过 $B$ 结果为 $l$ 字符串的集合。\n",
    "\n",
    "**CTC 假设输出的概率是（相对于输入）条件独立的**，因此有：\n",
    "$$p(\\pi|x) = \\prod y^t_{\\pi_t}, \\forall \\pi \\in L^{\\prime T}$$\n",
    "\n",
    "\n",
    "然而，直接按上式我们没有办理有效的计算似然值。下面用动态规划解决似然的计算及梯度计算, 涉及前向算法和后向算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 前向算法\n",
    "\n",
    "在前向及后向计算中，CTC 需要将输出字符串进行扩展。具体的，$(a_1,\\cdots,a_m)$ 每个字符之间及首尾分别插入 blank，即扩展为 $(\\%, a_1,\\%,a_2, \\%,\\cdots,\\%, a_m,\\%)$。下面的 $l$ 为原始字符串，$l^\\prime$ 指为扩展后的字符串。\n",
    "\n",
    "定义\n",
    "$$\n",
    "\\alpha_t(s) \\stackrel{def}{=} \\sum_{\\pi \\in N^T: B(\\pi_{1:t}) = l_{1:s}} \\prod_{t^\\prime=1}^t y^t_{\\pi^\\prime}\n",
    "$$\n",
    "\n",
    "显然有，\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha_1(1) = y_b^1,\\\\\n",
    "\\alpha_1(2) = y_{l_1}^1,\\\\\n",
    "\\alpha_1(s) = 0, \\forall s > 2\n",
    "\\end{align}\n",
    "$$\n",
    "根据 $\\alpha$ 的定义，有如下递归关系：\n",
    "$$\n",
    "\\alpha_t(s) = \\{  \\begin{array}{l}\n",
    "(\\alpha_{t-1}(s)+\\alpha_{t-1}(s-1)) y^t_{l^\\prime_s},\\  \\  \\    if\\  l^\\prime_s = b \\ or\\  l_{s-2}^\\prime = l_s^{\\prime}  \\\\\n",
    "(\\alpha_{t-1}(s)+\\alpha_{t-1}(s-1) + \\alpha_{t-1}(s-2)) y^t_{l^\\prime_s} \\ \\ otherwise\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Case 2\n",
    "递归公式中 case 2 是一般的情形。如图所示，$t$ 时刻字符为 $s$ 为 blank 时，它可能由于两种情况扩展而来：1）重复上一字符，即上个字符也是 a，2）字符发生转换，即上个字符是非 a 的字符。第二种情况又分为两种情形，2.1）上一字符是 blank；2.2）a 由非 blank 字符直接跳转而来（$B$） 操作中， blank 最终会被去掉，因此 blank 并不是必须的）。\n",
    "![](https://distill.pub/2017/ctc/assets/cost_regular.svg)\n",
    "**图2. 前向算法 Case 2 示例【[src](https://distill.pub/2017/ctc/)】**\n",
    "\n",
    "### 1.4.2 Case 1\n",
    "递归公式 case 1 是特殊的情形。\n",
    "如图所示，$t$ 时刻字符为 $s$ 为 blank 时，它只能由于两种情况扩展而来：1）重复上一字符，即上个字符也是 blank，2）字符发生转换，即上个字符是非 blank 字符。$t$ 时刻字符为 $s$ 为非 blank 时，类似于 case 2，但是这时两个相同字符之间的 blank 不能省略（否则无法区分\"aa\"和\"a\"），因此，也只有两种跳转情况。\n",
    "\n",
    "![](https://distill.pub/2017/ctc/assets/cost_no_skip.svg)\n",
    "**图3. 前向算法 Case 1 【[src](https://distill.pub/2017/ctc/)】**\n",
    "\n",
    "我们可以利用动态规划计算所有 $\\alpha$ 的值，算法时间和空间复杂度为 $O(T * L)$。\n",
    "\n",
    "似然的计算只涉及乘加运算，因此，CTC 的似然是可导的，可以尝试 tensorflow 或 pytorch 等具有自动求导功能的工具自动进行梯度计算。下面介绍如何手动高效的计算梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.46545113e-01 1.67574654e-01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [6.27300235e-02 7.13969720e-02 4.26370730e-02 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.51395174e-02 1.74287803e-02 2.75214373e-02 5.54036251e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.12040964e-03 4.61964998e-03 1.22337658e-02 4.68965079e-03\n",
      "  1.50787918e-03 1.03895167e-03 0.00000000e+00]\n",
      " [1.34152305e-03 8.51612635e-04 5.48713543e-03 1.64898136e-03\n",
      "  2.01779193e-03 1.37377693e-03 3.38261905e-04]\n",
      " [3.20028190e-04 3.76301179e-04 1.51214552e-03 1.22442454e-03\n",
      "  8.74730268e-04 1.06283215e-03 4.08416903e-04]\n",
      " [1.23322177e-04 1.01788478e-04 7.27708889e-04 4.00028082e-04\n",
      "  8.08904808e-04 5.40783712e-04 5.66942671e-04]\n",
      " [2.69673617e-05 3.70815141e-05 1.81389560e-04 1.85767281e-04\n",
      "  2.64362267e-04 3.82184328e-04 2.42231029e-04]\n",
      " [8.05153930e-06 7.40568461e-06 6.52280509e-05 4.24527009e-05\n",
      "  1.34393412e-04 1.47631121e-04 1.86429242e-04]\n",
      " [1.95126637e-06 3.64053019e-06 1.76025677e-05 2.53612828e-05\n",
      "  4.28581244e-05 5.31947855e-05 8.09585256e-05]\n",
      " [5.18984675e-07 1.37335633e-06 5.65009596e-06 1.05520069e-05\n",
      "  1.81445380e-05 1.87825719e-05 3.56811933e-05]\n",
      " [1.21116956e-07 3.82213679e-07 1.63908339e-06 3.27248912e-06\n",
      "  6.69699576e-06 7.59916314e-06 1.27103665e-05]]\n"
     ]
    }
   ],
   "source": [
    "def forward(y, labels):\n",
    "    T, V = y.shape\n",
    "    L = len(labels)\n",
    "    alpha = np.zeros([T, L])\n",
    "\n",
    "    # init\n",
    "    alpha[0, 0] = y[0, labels[0]]\n",
    "    alpha[0, 1] = y[0, labels[1]]\n",
    "\n",
    "    for t in range(1, T):\n",
    "        for i in range(L):\n",
    "            s = labels[i]\n",
    "\n",
    "            a = alpha[t - 1, i]\n",
    "            if i - 1 >= 0:\n",
    "                a += alpha[t - 1, i - 1]\n",
    "            if i - 2 >= 0 and s != 0 and s != labels[i - 2]:\n",
    "                a += alpha[t - 1, i - 2]\n",
    "\n",
    "            alpha[t, i] = a * y[t, s]\n",
    "\n",
    "    return alpha\n",
    "\n",
    "\n",
    "labels = [0, 3, 0, 3, 0, 4, 0]  # 0 for blank\n",
    "alpha = forward(y, labels)\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后可以得到似然 $p(l|x) = \\alpha_T(|l^\\prime|) + \\alpha_T(|l^\\prime|-1)$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.8181127117679896e-06\n"
     ]
    }
   ],
   "source": [
    "p = alpha[-1, labels[-1]] + alpha[-1, labels[-2]]\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 后向计算\n",
    "类似于前向计算，我们定义后向计算。\n",
    "首先定义\n",
    "$$\n",
    "\\beta_t(s)   \\stackrel{def}{=} \\sum_{\\pi \\in N^T: B(\\pi_{t:T}) = l_{s:|l|}} \\prod_{t^\\prime=t}^T y^t_{\\pi^\\prime}\n",
    "$$\n",
    "\n",
    "显然，\n",
    "$$\n",
    "\\begin{align}\n",
    "\\beta_T(|l^\\prime|) = y_b^T,\\\\\n",
    "\\beta_T(|l^\\prime|-1) = y_{l_{|l|}}^T,\\\\\n",
    "\\beta_T(s) = 0, \\forall s < |l^\\prime| - 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "易得如下递归关系：\n",
    "$$\n",
    "\\beta_t(s) = \\{  \\begin{array}{l}\n",
    "(\\beta_{t+1}(s)+\\beta_{t+1}(s+1)) y^t_{l^\\prime_s},\\  \\  \\    if\\  l^\\prime_s = b \\ or\\  l_{s+2}^\\prime = l_s^{\\prime}  \\\\\n",
    "(\\beta_{t+1}(s)+\\beta_{t+1}(s+1) + \\beta_{t+1}(s+2)) y^t_{l^\\prime_s} \n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.25636660e-05 7.74586366e-06 8.69559539e-06 3.30990037e-06\n",
      "  2.41325357e-06 4.30516936e-07 1.21116956e-07]\n",
      " [3.00418145e-05 2.09170784e-05 2.53062822e-05 9.96351200e-06\n",
      "  8.39236521e-06 1.39591874e-06 4.91256769e-07]\n",
      " [7.14014755e-05 4.66705755e-05 7.46535563e-05 2.48066359e-05\n",
      "  2.77113594e-05 5.27279259e-06 1.93076535e-06]\n",
      " [1.69926001e-04 1.25923340e-04 2.33240296e-04 7.60839197e-05\n",
      "  9.89830489e-05 1.58379311e-05 8.00005392e-06]\n",
      " [4.20893778e-04 2.03461048e-04 6.84292101e-04 1.72696845e-04\n",
      "  3.08627225e-04 5.50636993e-05 2.93943967e-05]\n",
      " [4.81953899e-04 8.10796738e-04 1.27731424e-03 8.24448952e-04\n",
      "  7.48161143e-04 1.99769340e-04 9.02831714e-05]\n",
      " [9.80428697e-04 1.03986915e-03 3.68556718e-03 1.66879393e-03\n",
      "  2.56724754e-03 5.68961868e-04 3.78457146e-04]\n",
      " [2.40870506e-04 2.30339872e-03 4.81028886e-03 4.75397134e-03\n",
      "  4.31752827e-03 2.34462771e-03 9.82118206e-04]\n",
      " [0.00000000e+00 1.10150469e-03 1.28817322e-02 9.11579592e-03\n",
      "  1.35011919e-02 6.24293419e-03 4.49124231e-03]\n",
      " [0.00000000e+00 0.00000000e+00 9.52648414e-03 3.36188472e-02\n",
      "  2.50664437e-02 2.01536701e-02 1.50427081e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 3.93092725e-02\n",
      "  4.25697510e-02 6.08622868e-02 6.20709492e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.60052748e-01 2.33372894e-01]]\n"
     ]
    }
   ],
   "source": [
    "def backward(y, labels):\n",
    "    T, V = y.shape\n",
    "    L = len(labels)\n",
    "    beta = np.zeros([T, L])\n",
    "\n",
    "    # init\n",
    "    beta[-1, -1] = y[-1, labels[-1]]\n",
    "    beta[-1, -2] = y[-1, labels[-2]]\n",
    "\n",
    "    for t in range(T - 2, -1, -1):\n",
    "        for i in range(L):\n",
    "            s = labels[i]\n",
    "\n",
    "            a = beta[t + 1, i]\n",
    "            if i + 1 < L:\n",
    "                a += beta[t + 1, i + 1]\n",
    "            if i + 2 < L and s != 0 and s != labels[i + 2]:\n",
    "                a += beta[t + 1, i + 2]\n",
    "\n",
    "            beta[t, i] = a * y[t, s]\n",
    "\n",
    "    return beta\n",
    "\n",
    "\n",
    "beta = backward(y, labels)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 梯度计算\n",
    "下面，我们利用前向、后者计算的 $\\alpha$ 和 $\\beta$ 来计算梯度。\n",
    "\n",
    "根据 $\\alpha$、$\\beta$ 的定义，我们有：\n",
    "$$\\alpha_t(s)\\beta_t(s) = \\sum_{\\pi \\in  B^{-1}(l):\\pi_t=l_s^\\prime} y^t_{l_s^\\prime} \\prod_{t=1}^T y^t_{\\pi_t} = y^t_{l_s^\\prime} \\cdot \\sum_{\\pi \\in  B^{-1}(l):\\pi_t=l_s^\\prime} \\prod_{t=1}^T y^t_{\\pi_t}$$\n",
    "则\n",
    "$$\n",
    "\\frac{\\alpha_t(s)\\beta_t(s)}{ y^t_{l_s^\\prime}} = \\sum_{\\pi \\in  B^{-1}(l):\\pi_t=l_s^\\prime} \\prod_{t=1}^T y^t_{\\pi_t}  = \\sum_{\\pi \\in  B^{-1}(l):\\pi_t=l_s^\\prime} p(\\pi|x) \n",
    "$$\n",
    "于是，可得似然\n",
    "$$\n",
    "p(l|x) = \\sum_{s=1}^{|l^\\prime|} \\sum_{\\pi \\in  B^{-1}(l):\\pi_t=l_s^\\prime} p(\\pi|x) =  \\sum_{s=1}^{|l^\\prime|}  \\frac{\\alpha_t(s)\\beta_t(s)}{ y^t_{l_s^\\prime}} \n",
    "$$\n",
    "\n",
    "\n",
    "为计算 $\\frac{\\partial p(l|x)}{\\partial y^t_k}$，观察上式右端求各项，仅有 $s=k$ 的项包含 $y^t_k$，因此，其他项的偏导都为零，不用考虑。于是有：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial p(l|x)}{\\partial y^t_k} = \\frac{\\partial \\frac{\\alpha_t(k)\\beta_t(k)}{ y^t_{k}} }{\\partial y^t_k} \n",
    "$$\n",
    "\n",
    "利用除法的求导准则有：\n",
    "$$\n",
    "\\frac{\\partial p(l|x)}{\\partial y^t_k} =  \\frac{\\frac{2 \\cdot \\alpha_t(k)\\beta_t(k)}{ y^t_{k}} \\cdot y^t_{k} -  \\alpha_t(k)\\beta_t(k) \\cdot 1}{{y^t_k}^2} = \\frac{\\alpha_t(k)\\beta_t(k)}{{y^t_k}^2}\n",
    "$$\n",
    "\n",
    "> 求导中，分子第一项是因为 $\\alpha(k)\\beta(k)$ 中包含为两个 $y^t_k$ 乘积项（即 ${y^t_k}^2$），其他均为与 $y^t_k$ 无关的常数。\n",
    "\n",
    "$l$ 中可能包含多个 $k$ 字符，它们计算的梯度要进行累加，因此，最后的梯度计算结果为：\n",
    "$$\n",
    "\\frac{\\partial p(l|x)}{\\partial y^t_k} = \\frac{1}{{y^t_k}^2} \\sum_{s \\in lab(l, k)} \\alpha_t(s)\\beta_t(s)\n",
    "$$\n",
    "其中，$lab(s)=\\{s: l_s^\\prime = k\\}$。\n",
    "\n",
    "一般我们优化似然函数的对数，因此，梯度计算如下：\n",
    "$$\n",
    "\\frac{\\partial \\ln(p(l|x))}{\\partial y^t_k} =\\frac{1}{p(l|x)} \\frac{\\partial p(l|x)}{\\partial y^t_k}\n",
    "$$\n",
    "其中，似然值在前向计算中已经求得： $p(l|x) = \\alpha_T(|l^\\prime|) + \\alpha_T(|l^\\prime|-1)$。\n",
    "\n",
    "对于给定训练集 $D$，待优化的目标函数为：\n",
    "$$\n",
    "O(D, N_w) = -\\sum_{(x,z)\\in D} \\ln(p(z|x))\n",
    "$$\n",
    "\n",
    "得到梯度后，我们可以利用任意优化方法（e.g. SGD， Adam）进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.50911241 0.         0.         2.27594441 0.        ]\n",
      " [2.25397118 0.         0.         2.47384957 0.        ]\n",
      " [2.65058465 0.         0.         2.77274592 0.        ]\n",
      " [2.46136916 0.         0.         2.29678159 0.02303985]\n",
      " [2.300259   0.         0.         2.37548238 0.10334851]\n",
      " [2.40271071 0.         0.         2.19860276 0.23513657]\n",
      " [1.68914157 0.         0.         1.78214377 0.51794046]\n",
      " [2.32536762 0.         0.         1.75750877 0.92477606]\n",
      " [1.92883907 0.         0.         1.45529832 1.44239844]\n",
      " [2.06219335 0.         0.         0.7568118  1.96405515]\n",
      " [2.07914466 0.         0.         0.33858403 2.35197258]\n",
      " [2.6816852  0.         0.         0.         2.3377753 ]]\n"
     ]
    }
   ],
   "source": [
    "def gradient(y, labels):\n",
    "    T, V = y.shape\n",
    "    L = len(labels)\n",
    "\n",
    "    alpha = forward(y, labels)\n",
    "    beta = backward(y, labels)\n",
    "    p = alpha[-1, -1] + alpha[-1, -2]\n",
    "\n",
    "    grad = np.zeros([T, V])\n",
    "    for t in range(T):\n",
    "        for s in range(V):\n",
    "            lab = [i for i, c in enumerate(labels) if c == s]\n",
    "            for i in lab:\n",
    "                grad[t, s] += alpha[t, i] * beta[t, i]\n",
    "            grad[t, s] /= y[t, s] ** 2\n",
    "\n",
    "    grad /= p\n",
    "    return grad\n",
    "\n",
    "\n",
    "grad = gradient(y, labels)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将基于前向-后向算法得到梯度与基于数值的梯度比较，以验证实现的正确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05\n",
      "1e-06\n",
      "[0, 3]：3.91e-06\n",
      "[1, 0]：3.61e-06\n",
      "[1, 3]：2.66e-06\n",
      "[2, 0]：2.67e-06\n",
      "[2, 3]：3.88e-06\n",
      "[3, 0]：4.71e-06\n",
      "[3, 3]：3.39e-06\n",
      "[4, 0]：1.24e-06\n",
      "[4, 3]：4.79e-06\n",
      "[5, 0]：1.57e-06\n",
      "[5, 3]：2.98e-06\n",
      "[6, 0]：5.03e-06\n",
      "[6, 3]：4.89e-06\n",
      "[7, 0]：1.05e-06\n",
      "[7, 4]：4.19e-06\n",
      "[8, 4]：5.57e-06\n",
      "[9, 0]：5.95e-06\n",
      "[9, 3]：3.85e-06\n",
      "[10, 0]：1.09e-06\n",
      "[10, 3]：1.53e-06\n",
      "[10, 4]：3.82e-06\n"
     ]
    }
   ],
   "source": [
    "def check_grad(y, labels, w=-1, v=-1, toleration=1e-3):\n",
    "    grad_1 = gradient(y, labels)[w, v]\n",
    "\n",
    "    delta = 1e-10\n",
    "    original = y[w, v]\n",
    "\n",
    "    y[w, v] = original + delta\n",
    "    alpha = forward(y, labels)\n",
    "    log_p1 = np.log(alpha[-1, -1] + alpha[-1, -2])\n",
    "\n",
    "    y[w, v] = original - delta\n",
    "    alpha = forward(y, labels)\n",
    "    log_p2 = np.log(alpha[-1, -1] + alpha[-1, -2])\n",
    "\n",
    "    y[w, v] = original\n",
    "\n",
    "    grad_2 = (log_p1 - log_p2) / (2 * delta)\n",
    "    if np.abs(grad_1 - grad_2) > toleration:\n",
    "        print('[%d, %d]：%.2e' % (w, v, np.abs(grad_1 - grad_2)))\n",
    "\n",
    "\n",
    "for toleration in [1e-5, 1e-6]:\n",
    "    print('%.e' % toleration)\n",
    "    for w in range(y.shape[0]):\n",
    "        for v in range(y.shape[1]):\n",
    "            check_grad(y, labels, w, v, toleration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，前向-后向及数值梯度两种方法计算的梯度差异都在 1e-5 以下，误差最多在 1e-6 的量级。这初步验证了前向-后向梯度计算方法原理和实现的正确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 logits 梯度\n",
    "在实际训练中，为了计算方便，可以将 CTC 和 softmax 的梯度计算合并，公式如下：\n",
    "$$\n",
    "\\frac{\\partial \\ln(p(l|x))}{\\partial y^t_k} = y^t_k - \\frac{1}{y^t_k \\cdot p(l|x)} \\sum_{s \\in lab(l, k)} \\alpha_t(s)\\beta_t(s)\n",
    "$$\n",
    "\n",
    "这是因为，softmax 的梯度反传公式为：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ln(p(l|x))}{\\partial u^t_k} = y^t_k (\\frac{\\partial \\ln(p(l|x))}{\\partial y^t_k}  - \\sum_{j=1}^{V} \\frac{\\partial \\ln(p(l|x))}{\\partial y^t_j} y^t_j)\n",
    "$$\n",
    "\n",
    "接合上面两式，有:\n",
    "$$\n",
    "\\frac{\\partial \\ln(p(l|x))}{\\partial u^t_k} = \\frac{1}{y^t_k p(l|x)} \\sum_{s \\in lab(l, k)} \\alpha_t(s)\\beta_t(s) - y^t_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5994150448506161e-15\n"
     ]
    }
   ],
   "source": [
    "def gradient_logits_naive(y, labels):\n",
    "    '''\n",
    "    gradient by back propagation\n",
    "    '''\n",
    "    y_grad = gradient(y, labels)\n",
    "\n",
    "    sum_y_grad = np.sum(y_grad * y, axis=1, keepdims=True)\n",
    "    u_grad = y * (y_grad - sum_y_grad)\n",
    "\n",
    "    return u_grad\n",
    "\n",
    "\n",
    "def gradient_logits(y, labels):\n",
    "    '''\n",
    "    '''\n",
    "    T, V = y.shape\n",
    "    L = len(labels)\n",
    "\n",
    "    alpha = forward(y, labels)\n",
    "    beta = backward(y, labels)\n",
    "    p = alpha[-1, -1] + alpha[-1, -2]\n",
    "\n",
    "    u_grad = np.zeros([T, V])\n",
    "    for t in range(T):\n",
    "        for s in range(V):\n",
    "            lab = [i for i, c in enumerate(labels) if c == s]\n",
    "            for i in lab:\n",
    "                u_grad[t, s] += alpha[t, i] * beta[t, i]\n",
    "            u_grad[t, s] /= y[t, s] * p\n",
    "\n",
    "    u_grad -= y\n",
    "    return u_grad\n",
    "\n",
    "\n",
    "grad_l = gradient_logits_naive(y, labels)\n",
    "grad_2 = gradient_logits(y, labels)\n",
    "\n",
    "print(np.sum(np.abs(grad_l - grad_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同上，我们利用数值梯度来初步检验梯度计算的正确性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05\n",
      "[0, 6]：-8.00e-02, -8.00e-02, 1.03e-05\n",
      "[4, 0]：4.29e-01, 4.29e-01, 1.10e-05\n",
      "[5, 6]：-7.59e-02, -7.59e-02, 1.22e-05\n",
      "[6, 2]：-1.38e-01, -1.38e-01, 1.23e-05\n",
      "[6, 3]：3.33e-01, 3.33e-01, 1.02e-05\n",
      "1e-06\n",
      "[0, 0]：3.88e-01, 3.88e-01, 7.03e-06\n",
      "[0, 1]：-1.59e-01, -1.59e-01, 2.78e-06\n",
      "[0, 2]：-8.89e-02, -8.89e-02, 3.47e-06\n",
      "[0, 3]：4.57e-01, 4.57e-01, 1.64e-06\n",
      "[0, 4]：-6.32e-02, -6.32e-02, 7.19e-06\n",
      "[0, 5]：-7.98e-02, -7.98e-02, 5.46e-06\n",
      "[0, 6]：-8.00e-02, -8.00e-02, 1.03e-05\n",
      "[0, 7]：-1.32e-01, -1.32e-01, 2.21e-06\n",
      "[0, 8]：-1.04e-01, -1.04e-01, 7.75e-06\n",
      "[0, 9]：-1.38e-01, -1.38e-01, 5.95e-06\n",
      "[1, 0]：3.41e-01, 3.41e-01, 2.79e-06\n",
      "[1, 1]：-1.18e-01, -1.18e-01, 6.08e-06\n",
      "[1, 3]：5.04e-01, 5.04e-01, 4.06e-06\n",
      "[1, 4]：-9.96e-02, -9.96e-02, 5.77e-06\n",
      "[1, 5]：-8.22e-02, -8.22e-02, 4.03e-06\n",
      "[1, 6]：-9.46e-02, -9.46e-02, 4.49e-06\n",
      "[1, 7]：-1.49e-01, -1.49e-01, 3.96e-06\n",
      "[1, 8]：-1.24e-01, -1.24e-01, 4.96e-06\n",
      "[1, 9]：-7.48e-02, -7.47e-02, 5.94e-06\n",
      "[2, 0]：3.29e-01, 3.29e-01, 3.47e-06\n",
      "[2, 1]：-9.42e-02, -9.42e-02, 1.63e-06\n",
      "[2, 2]：-9.17e-02, -9.17e-02, 4.47e-06\n",
      "[2, 3]：4.50e-01, 4.50e-01, 2.14e-06\n",
      "[2, 5]：-1.07e-01, -1.07e-01, 6.33e-06\n",
      "[2, 6]：-5.42e-02, -5.42e-02, 1.71e-06\n",
      "[2, 7]：-9.68e-02, -9.68e-02, 7.69e-06\n",
      "[2, 9]：-1.21e-01, -1.21e-01, 9.06e-06\n",
      "[3, 0]：4.42e-01, 4.42e-01, 9.21e-06\n",
      "[3, 1]：-6.71e-02, -6.71e-02, 5.75e-06\n",
      "[3, 2]：-1.16e-01, -1.16e-01, 5.26e-06\n",
      "[3, 3]：4.03e-01, 4.03e-01, 6.39e-06\n",
      "[3, 4]：-1.07e-01, -1.07e-01, 2.42e-06\n",
      "[3, 5]：-1.25e-01, -1.25e-01, 8.90e-06\n",
      "[3, 6]：-1.17e-01, -1.17e-01, 2.08e-06\n",
      "[3, 7]：-1.32e-01, -1.32e-01, 2.21e-06\n",
      "[3, 8]：-6.90e-02, -6.90e-02, 1.72e-06\n",
      "[3, 9]：-1.13e-01, -1.13e-01, 6.68e-06\n",
      "[4, 0]：4.29e-01, 4.29e-01, 1.10e-05\n",
      "[4, 1]：-7.17e-02, -7.17e-02, 7.76e-06\n",
      "[4, 3]：3.25e-01, 3.25e-01, 2.93e-06\n",
      "[4, 4]：-5.91e-02, -5.91e-02, 1.88e-06\n",
      "[4, 5]：-7.78e-02, -7.78e-02, 5.82e-06\n",
      "[4, 6]：-9.08e-02, -9.08e-02, 8.04e-06\n",
      "[4, 7]：-1.12e-01, -1.12e-01, 2.11e-06\n",
      "[4, 8]：-1.26e-01, -1.26e-01, 5.92e-06\n",
      "[4, 9]：-1.40e-01, -1.40e-01, 4.89e-06\n",
      "[5, 0]：1.86e-01, 1.86e-01, 6.60e-06\n",
      "[5, 2]：-9.52e-02, -9.52e-02, 5.01e-06\n",
      "[5, 3]：4.97e-01, 4.97e-01, 6.92e-06\n",
      "[5, 4]：2.50e-02, 2.50e-02, 5.20e-06\n",
      "[5, 5]：-7.93e-02, -7.93e-02, 2.78e-06\n",
      "[5, 6]：-7.59e-02, -7.59e-02, 1.22e-05\n",
      "[5, 7]：-1.05e-01, -1.05e-01, 9.53e-06\n",
      "[5, 8]：-1.25e-01, -1.25e-01, 9.07e-06\n",
      "[5, 9]：-6.74e-02, -6.74e-02, 3.08e-06\n",
      "[6, 0]：1.49e-01, 1.49e-01, 5.17e-06\n",
      "[6, 1]：-6.67e-02, -6.66e-02, 4.40e-06\n",
      "[6, 2]：-1.38e-01, -1.38e-01, 1.23e-05\n",
      "[6, 3]：3.33e-01, 3.33e-01, 1.02e-05\n",
      "[6, 4]：1.72e-01, 1.72e-01, 7.79e-06\n",
      "[6, 6]：-6.73e-02, -6.73e-02, 7.13e-06\n",
      "[6, 7]：-6.08e-02, -6.08e-02, 8.58e-06\n",
      "[6, 8]：-9.98e-02, -9.98e-02, 6.14e-06\n",
      "[6, 9]：-1.31e-01, -1.31e-01, 8.89e-06\n",
      "[7, 0]：2.39e-01, 2.39e-01, 2.53e-06\n",
      "[7, 2]：-1.61e-01, -1.61e-01, 1.78e-06\n",
      "[7, 3]：1.02e-01, 1.02e-01, 4.73e-06\n",
      "[7, 4]：3.63e-01, 3.63e-01, 7.51e-06\n",
      "[7, 5]：-6.02e-02, -6.02e-02, 4.63e-06\n",
      "[7, 6]：-1.05e-01, -1.05e-01, 3.29e-06\n",
      "[7, 8]：-8.83e-02, -8.83e-02, 4.16e-06\n",
      "[7, 9]：-6.05e-02, -6.05e-02, 2.89e-06\n",
      "[8, 0]：2.92e-01, 2.92e-01, 1.38e-06\n",
      "[8, 1]：-9.70e-02, -9.70e-02, 2.27e-06\n",
      "[8, 2]：-9.87e-02, -9.87e-02, 4.36e-06\n",
      "[8, 4]：5.07e-01, 5.07e-01, 2.30e-06\n",
      "[8, 5]：-1.28e-01, -1.28e-01, 7.41e-06\n",
      "[8, 6]：-1.09e-01, -1.09e-01, 6.32e-06\n",
      "[8, 7]：-1.20e-01, -1.20e-01, 6.01e-06\n",
      "[8, 8]：-7.00e-02, -7.00e-02, 1.44e-06\n",
      "[8, 9]：-1.55e-01, -1.55e-01, 2.47e-06\n",
      "[9, 0]：4.90e-01, 4.90e-01, 2.04e-06\n",
      "[9, 1]：-7.69e-02, -7.69e-02, 1.54e-06\n",
      "[9, 2]：-9.72e-02, -9.72e-02, 9.59e-06\n",
      "[9, 4]：2.61e-01, 2.61e-01, 2.16e-06\n",
      "[9, 5]：-9.27e-02, -9.27e-02, 5.07e-06\n",
      "[9, 6]：-7.70e-02, -7.70e-02, 1.03e-06\n",
      "[9, 7]：-8.30e-02, -8.30e-02, 5.42e-06\n",
      "[9, 8]：-1.17e-01, -1.17e-01, 4.26e-06\n",
      "[9, 9]：-1.39e-01, -1.39e-01, 3.53e-06\n"
     ]
    }
   ],
   "source": [
    "def check_grad_logits(x, labels, w=-1, v=-1, toleration=1e-3):\n",
    "    grad_1 = gradient_logits(softmax(x), labels)[w, v]\n",
    "\n",
    "    delta = 1e-10\n",
    "    original = x[w, v]\n",
    "\n",
    "    x[w, v] = original + delta\n",
    "    y = softmax(x)\n",
    "    alpha = forward(y, labels)\n",
    "    log_p1 = np.log(alpha[-1, -1] + alpha[-1, -2])\n",
    "\n",
    "    x[w, v] = original - delta\n",
    "    y = softmax(x)\n",
    "    alpha = forward(y, labels)\n",
    "    log_p2 = np.log(alpha[-1, -1] + alpha[-1, -2])\n",
    "\n",
    "    x[w, v] = original\n",
    "\n",
    "    grad_2 = (log_p1 - log_p2) / (2 * delta)\n",
    "    if np.abs(grad_1 - grad_2) > toleration:\n",
    "        print('[%d, %d]：%.2e, %.2e, %.2e' %\n",
    "              (w, v, grad_1, grad_2, np.abs(grad_1 - grad_2)))\n",
    "\n",
    "\n",
    "np.random.seed(1111)\n",
    "x = np.random.random([10, 10])\n",
    "for toleration in [1e-5, 1e-6]:\n",
    "    print('%.e' % toleration)\n",
    "    for w in range(x.shape[0]):\n",
    "        for v in range(x.shape[1]):\n",
    "            check_grad_logits(x, labels, w, v, toleration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 数值稳定性\n",
    "\n",
    "CTC 的训练过程面临数值下溢的风险，特别是序列较大的情况下。下面介绍两种数值上稳定的工程优化方法：1）log 域（许多 CRF 实现的常用方法）；2）scale 技巧（原始论文 [1] 使用的方法）。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 log 域计算\n",
    "\n",
    "log 计算涉及 logsumexp 操作。\n",
    "[经验表明](https://github.com/baidu-research/warp-ctc)，在 log 域计算，即使使用单精度，也表现出良好的数值稳定性，可以有效避免下溢的风险。稳定性的代价是增加了运算的复杂性——原始实现只涉及乘加运算，log 域实现则需要对数和指数运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-5f65301d1c3d>:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ninf = -np.float('inf')\n"
     ]
    }
   ],
   "source": [
    "ninf = -np.float('inf')\n",
    "\n",
    "\n",
    "def _logsumexp(a, b):\n",
    "    '''\n",
    "    np.log(np.exp(a) + np.exp(b))\n",
    "\n",
    "    '''\n",
    "\n",
    "    if a < b:\n",
    "        a, b = b, a\n",
    "\n",
    "    if b == ninf:\n",
    "        return a\n",
    "    else:\n",
    "        return a + np.log(1 + np.exp(b - a))\n",
    "\n",
    "\n",
    "def logsumexp(*args):\n",
    "    '''\n",
    "    from scipy.special import logsumexp\n",
    "    logsumexp(args)\n",
    "    '''\n",
    "    res = args[0]\n",
    "    for e in args[1:]:\n",
    "        res = _logsumexp(res, e)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 log 域前向算法\n",
    "基于 log 的前向算法实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.608819359419081e-17\n"
     ]
    }
   ],
   "source": [
    "def forward_log(log_y, labels):\n",
    "    T, V = log_y.shape\n",
    "    L = len(labels)\n",
    "    log_alpha = np.ones([T, L]) * ninf\n",
    "\n",
    "    # init\n",
    "    log_alpha[0, 0] = log_y[0, labels[0]]\n",
    "    log_alpha[0, 1] = log_y[0, labels[1]]\n",
    "\n",
    "    for t in range(1, T):\n",
    "        for i in range(L):\n",
    "            s = labels[i]\n",
    "\n",
    "            a = log_alpha[t - 1, i]\n",
    "            if i - 1 >= 0:\n",
    "                a = logsumexp(a, log_alpha[t - 1, i - 1])\n",
    "            if i - 2 >= 0 and s != 0 and s != labels[i - 2]:\n",
    "                a = logsumexp(a, log_alpha[t - 1, i - 2])\n",
    "\n",
    "            log_alpha[t, i] = a + log_y[t, s]\n",
    "\n",
    "    return log_alpha\n",
    "\n",
    "\n",
    "log_alpha = forward_log(np.log(y), labels)\n",
    "alpha = forward(y, labels)\n",
    "print(np.sum(np.abs(np.exp(log_alpha) - alpha)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 log 域后向算法\n",
    "基于 log 的后向算法实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1039994500452056e-16\n"
     ]
    }
   ],
   "source": [
    "def backward_log(log_y, labels):\n",
    "    T, V = log_y.shape\n",
    "    L = len(labels)\n",
    "    log_beta = np.ones([T, L]) * ninf\n",
    "\n",
    "    # init\n",
    "    log_beta[-1, -1] = log_y[-1, labels[-1]]\n",
    "    log_beta[-1, -2] = log_y[-1, labels[-2]]\n",
    "\n",
    "    for t in range(T - 2, -1, -1):\n",
    "        for i in range(L):\n",
    "            s = labels[i]\n",
    "\n",
    "            a = log_beta[t + 1, i]\n",
    "            if i + 1 < L:\n",
    "                a = logsumexp(a, log_beta[t + 1, i + 1])\n",
    "            if i + 2 < L and s != 0 and s != labels[i + 2]:\n",
    "                a = logsumexp(a, log_beta[t + 1, i + 2])\n",
    "\n",
    "            log_beta[t, i] = a + log_y[t, s]\n",
    "\n",
    "    return log_beta\n",
    "\n",
    "\n",
    "log_beta = backward_log(np.log(y), labels)\n",
    "beta = backward(y, labels)\n",
    "print(np.sum(np.abs(np.exp(log_beta) - beta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 log 域梯度计算\n",
    "在前向、后向基础上，也可以在 log 域上计算梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9758808184918735e-14\n"
     ]
    }
   ],
   "source": [
    "def gradient_log(log_y, labels):\n",
    "    T, V = log_y.shape\n",
    "    L = len(labels)\n",
    "\n",
    "    log_alpha = forward_log(log_y, labels)\n",
    "    log_beta = backward_log(log_y, labels)\n",
    "    log_p = logsumexp(log_alpha[-1, -1], log_alpha[-1, -2])\n",
    "\n",
    "    log_grad = np.ones([T, V]) * ninf\n",
    "    for t in range(T):\n",
    "        for s in range(V):\n",
    "            lab = [i for i, c in enumerate(labels) if c == s]\n",
    "            for i in lab:\n",
    "                log_grad[t, s] = logsumexp(\n",
    "                    log_grad[t, s], log_alpha[t, i] + log_beta[t, i])\n",
    "            log_grad[t, s] -= 2 * log_y[t, s]\n",
    "\n",
    "    log_grad -= log_p\n",
    "    return log_grad\n",
    "\n",
    "\n",
    "log_grad = gradient_log(np.log(y), labels)\n",
    "grad = gradient(y, labels)\n",
    "# print(log_grad)\n",
    "# print(grad)\n",
    "print(np.sum(np.abs(np.exp(log_grad) - grad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 scale\n",
    "\n",
    "### 2.2.1 前向算法\n",
    "\n",
    "为了下溢，在前向算法的每个时刻，都对计算出的 $\\alpha$ 的范围进行缩放：\n",
    "$$\n",
    "C_t \\stackrel{def}{=} \\sum_s\\alpha_t(s)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\alpha}_t = \\frac{\\alpha_t(s)}{C_t}\n",
    "$$\n",
    "\n",
    "缩放后的 $\\alpha$，不会随着时刻的积累变得太小。$\\hat{\\alpha}$ 替代 $\\alpha$，进行下一时刻的迭代。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def forward_scale(y, labels):\n",
    "    T, V = y.shape\n",
    "    L = len(labels)\n",
    "    alpha_scale = np.zeros([T, L])\n",
    "\n",
    "    # init\n",
    "    alpha_scale[0, 0] = y[0, labels[0]]\n",
    "    alpha_scale[0, 1] = y[0, labels[1]]\n",
    "    Cs = []\n",
    "\n",
    "    C = np.sum(alpha_scale[0])\n",
    "    alpha_scale[0] /= C\n",
    "    Cs.append(C)\n",
    "\n",
    "    for t in range(1, T):\n",
    "        for i in range(L):\n",
    "            s = labels[i]\n",
    "\n",
    "            a = alpha_scale[t - 1, i]\n",
    "            if i - 1 >= 0:\n",
    "                a += alpha_scale[t - 1, i - 1]\n",
    "            if i - 2 >= 0 and s != 0 and s != labels[i - 2]:\n",
    "                a += alpha_scale[t - 1, i - 2]\n",
    "\n",
    "            alpha_scale[t, i] = a * y[t, s]\n",
    "\n",
    "        C = np.sum(alpha_scale[t])\n",
    "        alpha_scale[t] /= C\n",
    "        Cs.append(C)\n",
    "\n",
    "    return alpha_scale, Cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于进行了缩放，最后计算概率时要时行补偿：\n",
    "$$\n",
    "p(l|x) = \\alpha_T(|l^\\prime|) + \\alpha_T(|l^\\prime|-1) = (\\hat\\alpha_T(|l^\\prime|) + \\hat\\alpha_T(|l^\\prime|-1) * \\prod_{t=1}^T C_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ln(p(l|x)) = \\sum_t^T\\ln(C_t) + \\ln(\\hat\\alpha_T(|l^\\prime|) + \\hat\\alpha_T(|l^\\prime|-1))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-13.202925982240107 -13.202925982240107 0.0\n"
     ]
    }
   ],
   "source": [
    "labels = [0, 1, 2, 0]  # 0 for blank\n",
    "\n",
    "alpha_scale, Cs = forward_scale(y, labels)\n",
    "log_p = np.sum(np.log(Cs)) + \\\n",
    "    np.log(alpha_scale[-1][labels[-1]] + alpha_scale[-1][labels[-2]])\n",
    "\n",
    "alpha = forward(y, labels)\n",
    "p = alpha[-1, labels[-1]] + alpha[-1, labels[-2]]\n",
    "\n",
    "print(np.log(p), log_p, np.log(p) - log_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 后向算法\n",
    "后向算法缩放类似于前向算法，公式如下：\n",
    "\n",
    "$$\n",
    "D_t \\stackrel{def}{=} \\sum_s\\beta_t(s)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_t = \\frac{\\beta_t(s)}{D_t}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.71362347 0.18910147 0.07964328 0.01763178]\n",
      " [0.70165268 0.15859852 0.11849423 0.02125457]\n",
      " [0.67689676 0.165374   0.13221504 0.02551419]\n",
      " [0.71398181 0.11936432 0.13524265 0.03141122]\n",
      " [0.70769657 0.13093688 0.12447135 0.0368952 ]\n",
      " [0.63594568 0.1790638  0.14250065 0.04248987]\n",
      " [0.63144322 0.1806382  0.13366043 0.05425815]\n",
      " [0.33926289 0.35149591 0.24988622 0.05935497]\n",
      " [0.30303623 0.26644554 0.33088584 0.0996324 ]\n",
      " [0.12510056 0.3297143  0.3956509  0.14953425]\n",
      " [0.         0.22078343 0.5153114  0.26390517]\n",
      " [0.         0.         0.550151   0.449849  ]]\n"
     ]
    }
   ],
   "source": [
    "def backward_scale(y, labels):\n",
    "    T, V = y.shape\n",
    "    L = len(labels)\n",
    "    beta_scale = np.zeros([T, L])\n",
    "\n",
    "    # init\n",
    "    beta_scale[-1, -1] = y[-1, labels[-1]]\n",
    "    beta_scale[-1, -2] = y[-1, labels[-2]]\n",
    "\n",
    "    Ds = []\n",
    "\n",
    "    D = np.sum(beta_scale[-1, :])\n",
    "    beta_scale[-1] /= D\n",
    "    Ds.append(D)\n",
    "\n",
    "    for t in range(T - 2, -1, -1):\n",
    "        for i in range(L):\n",
    "            s = labels[i]\n",
    "\n",
    "            a = beta_scale[t + 1, i]\n",
    "            if i + 1 < L:\n",
    "                a += beta_scale[t + 1, i + 1]\n",
    "            if i + 2 < L and s != 0 and s != labels[i + 2]:\n",
    "                a += beta_scale[t + 1, i + 2]\n",
    "\n",
    "            beta_scale[t, i] = a * y[t, s]\n",
    "\n",
    "        D = np.sum(beta_scale[t])\n",
    "        beta_scale[t] /= D\n",
    "        Ds.append(D)\n",
    "\n",
    "    return beta_scale, Ds[::-1]\n",
    "\n",
    "\n",
    "beta_scale, Ds = backward_scale(y, labels)\n",
    "print(beta_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 梯度计算\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ln(p(l|x))}{\\partial y^t_k} = \\frac{1}{p(l|x)} \\frac{\\partial p(l|x)}{\\partial y^t_k} = \\frac{1}{p(l|x)} \\frac{1}{{y^t_k}^2} \\sum_{s \\in lab(l, k)} \\alpha_t(s)\\beta_t(s) \n",
    "$$\n",
    "\n",
    "考虑到\n",
    "$$\n",
    "p(l|x) =  \\sum_{s=1}^{|l^\\prime|}  \\frac{\\alpha_t(s)\\beta_t(s)}{ y^t_{l_s^\\prime}} \n",
    "$$\n",
    "以及\n",
    "$$\n",
    "\\alpha_t(s) = \\hat\\alpha_t(s) \\cdot \\prod_{k=1}^t C_k\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_t(s) = \\hat\\beta_t(s) \\cdot \\prod_{k=t}^T D_k\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ln(p(l|x))}{\\partial y^t_k} = \\frac{1}{\\sum_{s=1}^{|l^\\prime|} \\frac{\\hat\\alpha_t(s)  \\hat\\beta_t(s)}{y^t_{l^\\prime_s}}} \\frac{1}{{y^t_k}^2} \\sum_{s \\in lab(l, k)} \\hat\\alpha_t(s)  \\hat\\beta_t(s)\n",
    "$$\n",
    "\n",
    "式中最右项中的各个部分我们都已经求得。梯度计算实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.862566070964249e-15\n"
     ]
    }
   ],
   "source": [
    "def gradient_scale(y, labels):\n",
    "    T, V = y.shape\n",
    "    L = len(labels)\n",
    "\n",
    "    alpha_scale, _ = forward_scale(y, labels)\n",
    "    beta_scale, _ = backward_scale(y, labels)\n",
    "\n",
    "    grad = np.zeros([T, V])\n",
    "    for t in range(T):\n",
    "        for s in range(V):\n",
    "            lab = [i for i, c in enumerate(labels) if c == s]\n",
    "            for i in lab:\n",
    "                grad[t, s] += alpha_scale[t, i] * beta_scale[t, i]\n",
    "            grad[t, s] /= y[t, s] ** 2\n",
    "\n",
    "        # normalize factor\n",
    "        z = 0\n",
    "        for i in range(L):\n",
    "            z += alpha_scale[t, i] * beta_scale[t, i] / y[t, labels[i]]\n",
    "        grad[t] /= z\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "labels = [0, 3, 0, 3, 0, 4, 0]  # 0 for blank\n",
    "grad_1 = gradient_scale(y, labels)\n",
    "grad_2 = gradient(y, labels)\n",
    "print(np.sum(np.abs(grad_1 - grad_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 logits 梯度\n",
    "类似于 y 梯度的推导，logits 梯度计算公式如下：\n",
    "$$\n",
    "\\frac{\\partial \\ln(p(l|x))}{\\partial u^t_k} = \\frac{1}{y^t_k Z_t} \\sum_{s \\in lab(l, k)} \\hat\\alpha_t(s)\\hat\\beta_t(s) - y^t_k\n",
    "$$\n",
    "其中，\n",
    "$$\n",
    "Z_t \\stackrel{def}{=} \\sum_{s=1}^{|l^\\prime|} \\frac{\\hat\\alpha_t(s)\\hat\\beta_t(s)}{y^t_{l^\\prime_s}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 解码\n",
    "训练和的 $N_w$ 可以用来预测新的样本输入对应的输出字符串，这涉及到解码。\n",
    "按照最大似然准则，最优的解码结果为：\n",
    "$$\n",
    "h(x) = \\underset{l \\in L^{\\le T}}{\\mathrm{argmax}}\\ p(l|x)\n",
    "$$\n",
    "\n",
    "然而，上式不存在已知的高效解法。下面介绍几种实用的近似破解码方法。\n",
    "\n",
    "## 3.1 贪心搜索 （greedy search）\n",
    "虽然 $p(l|x)$ 难以有效的计算，但是由于 CTC 的独立性假设，对于某个具体的字符串 $\\pi$（去 blank 前），确容易计算：\n",
    "$$\n",
    "p(\\pi|x) = \\prod_{k=1}^T p(\\pi_k|x)\n",
    "$$\n",
    "\n",
    "因此，我们放弃寻找使 $p(l|x)$ 最大的字符串，退而寻找一个使 $p(\\pi|x)$ 最大的字符串，即：\n",
    "\n",
    "$$\n",
    "h(x) \\approx B(\\pi^\\star)\n",
    "$$\n",
    "其中，\n",
    "$$\n",
    "\\pi^\\star = \\underset{\\pi \\in N^T}{\\mathrm{argmax}}\\ p(\\pi|x)\n",
    "$$\n",
    "\n",
    "简化后，解码过程（构造 $\\pi^\\star$）变得非常简单（基于独立性假设）： 在每个时刻输出概率最大的字符:\n",
    "$$\n",
    "\\pi^\\star = cat_{t=1}^T(\\underset{s \\in L^\\prime}{\\mathrm{argmax}}\\ y^t_s)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 5 5 5 5 1 5 3 4 4 3 0 4 5 0 3 1 3 3]\n",
      "[1, 3, 5, 1, 5, 3, 4, 3, 4, 5, 3, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "def remove_blank(labels, blank=0):\n",
    "    new_labels = []\n",
    "\n",
    "    # combine duplicate\n",
    "    previous = None\n",
    "    for l in labels:\n",
    "        if l != previous:\n",
    "            new_labels.append(l)\n",
    "            previous = l\n",
    "\n",
    "    # remove blank\n",
    "    new_labels = [l for l in new_labels if l != blank]\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "def insert_blank(labels, blank=0):\n",
    "    new_labels = [blank]\n",
    "    for l in labels:\n",
    "        new_labels += [l, blank]\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "def greedy_decode(y, blank=0):\n",
    "    raw_rs = np.argmax(y, axis=1)\n",
    "    rs = remove_blank(raw_rs, blank)\n",
    "    return raw_rs, rs\n",
    "\n",
    "\n",
    "np.random.seed(1111)\n",
    "y = softmax(np.random.random([20, 6]))\n",
    "rr, rs = greedy_decode(y)\n",
    "print(rr)\n",
    "print(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 束搜索（Beam Search）\n",
    "显然，贪心搜索的性能非常受限。例如，它不能给出除最优路径之外的其他其优路径。很多时候，如果我们能拿到 nbest 的路径，后续可以利用其他信息来进一步优化搜索的结果。束搜索能近似找出 top 最优的若干条路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 1, 5, 3, 4, 3, 4, 5, 3, 1, 3] -29.261797539205567\n",
      "[1, 3, 5, 1, 5, 3, 4, 3, 3, 5, 3, 1, 3] -29.279020152518033\n",
      "[1, 3, 5, 1, 5, 3, 4, 2, 3, 4, 5, 3, 1, 3] -29.300726142201842\n",
      "[1, 5, 1, 5, 3, 4, 3, 4, 5, 3, 1, 3] -29.310307014773972\n",
      "[1, 3, 5, 1, 5, 3, 4, 2, 3, 3, 5, 3, 1, 3] -29.31794875551431\n",
      "[1, 5, 1, 5, 3, 4, 3, 3, 5, 3, 1, 3] -29.327529628086438\n",
      "[1, 3, 5, 1, 5, 4, 3, 4, 5, 3, 1, 3] -29.331572723457334\n",
      "[1, 3, 5, 5, 1, 5, 3, 4, 3, 4, 5, 3, 1, 3] -29.33263180992451\n",
      "[1, 3, 5, 4, 1, 5, 3, 4, 3, 4, 5, 3, 1, 3] -29.334649090836038\n",
      "[1, 3, 5, 1, 5, 3, 4, 3, 4, 5, 3, 1, 3] -29.33969505198154\n",
      "[1, 3, 5, 2, 1, 5, 3, 4, 3, 4, 5, 3, 1, 3] -29.339823066915415\n",
      "[1, 3, 5, 1, 5, 4, 3, 3, 5, 3, 1, 3] -29.3487953367698\n",
      "[1, 5, 1, 5, 3, 4, 2, 3, 4, 5, 3, 1, 3] -29.349235617770248\n",
      "[1, 3, 5, 5, 1, 5, 3, 4, 3, 3, 5, 3, 1, 3] -29.349854423236977\n",
      "[1, 3, 5, 1, 5, 3, 4, 3, 4, 5, 3, 3] -29.350803198551016\n",
      "[1, 3, 5, 4, 1, 5, 3, 4, 3, 3, 5, 3, 1, 3] -29.351871704148504\n",
      "[1, 3, 5, 1, 5, 3, 4, 3, 3, 5, 3, 1, 3] -29.356917665294006\n",
      "[1, 3, 5, 2, 1, 5, 3, 4, 3, 3, 5, 3, 1, 3] -29.35704568022788\n",
      "[1, 3, 5, 1, 5, 3, 4, 5, 4, 5, 3, 1, 3] -29.363802591012263\n",
      "[1, 5, 1, 5, 3, 4, 2, 3, 3, 5, 3, 1, 3] -29.366458231082714\n"
     ]
    }
   ],
   "source": [
    "def beam_decode(y: int,  beam_size: int = 10) -> tuple:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y: 概率矩阵.\n",
    "        beta_size: 束搜索路径大小.\n",
    "        \n",
    "    Return:\n",
    "        返回束搜索后的路径.\n",
    "    \"\"\"\n",
    "    T, V = y.shape\n",
    "    log_y = np.log(y)\n",
    "\n",
    "    beam = [([], 0)]\n",
    "    for t in range(T):  # for every timestep\n",
    "        new_beam = []\n",
    "        # 路径大小\n",
    "        for prefix, score in beam:\n",
    "            for i in range(V):  # for every state\n",
    "                new_prefix = prefix + [i]\n",
    "                new_score = score + log_y[t, i]\n",
    "\n",
    "                new_beam.append((new_prefix, new_score))\n",
    "\n",
    "        # top beam_size\n",
    "        new_beam.sort(key=lambda x: x[1], reverse=True)\n",
    "        beam = new_beam[:beam_size]\n",
    "\n",
    "    return beam\n",
    "\n",
    "\n",
    "np.random.seed(1111)\n",
    "y = softmax(np.random.random([20, 6]))\n",
    "beam = beam_decode(y, beam_size=100)\n",
    "for string, score in beam[:20]:\n",
    "    print(remove_blank(string), score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 前缀束搜索（Prefix Beam Search）\n",
    "直接的束搜索的一个问题是，在保存的 top N 条路径中，可能存在多条实际上是同一结果（经过去重复、去 blank 操作）。这减少了搜索结果的多样性。下面介绍的前缀搜索方法，在搜索过程中不断的合并相同的前缀[2]。参考 [gist](https://gist.github.com/awni/56369a90d03953e370f3964c826ed4b0)，前缀束搜索实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 5, 4, 1, 3, 4, 5, 2, 3], (-18.189863809114193, -17.613677981426175))\n",
      "([1, 5, 4, 5, 3, 4, 5, 2, 3], (-18.19636512622969, -17.621013424585406))\n",
      "([1, 5, 4, 1, 3, 4, 5, 1, 3], (-18.317018960331531, -17.666629973270073))\n",
      "([1, 5, 4, 5, 3, 4, 5, 1, 3], (-18.323388267369936, -17.674125139073176))\n",
      "([1, 5, 4, 1, 3, 4, 3, 2, 3], (-18.415808498759556, -17.862744326248826))\n",
      "([1, 5, 4, 1, 3, 4, 3, 5, 3], (-18.366422766638632, -17.898463479112884))\n",
      "([1, 5, 4, 5, 3, 4, 3, 2, 3], (-18.42224294936932, -17.870025672291458))\n",
      "([1, 5, 4, 5, 3, 4, 3, 5, 3], (-18.372199113900191, -17.905130493229173))\n",
      "([1, 5, 4, 1, 3, 4, 5, 4, 3], (-18.457066311773847, -17.880630315602037))\n",
      "([1, 5, 4, 5, 3, 4, 5, 4, 3], (-18.462614293487096, -17.88759583852546))\n",
      "([1, 5, 4, 1, 3, 4, 5, 3, 2], (-18.458941701567706, -17.951422824358747))\n",
      "([1, 5, 4, 5, 3, 4, 5, 3, 2], (-18.464527031120184, -17.958629487208658))\n",
      "([1, 5, 4, 1, 3, 4, 3, 1, 3], (-18.540857550725587, -17.920589910093689))\n",
      "([1, 5, 4, 5, 3, 4, 3, 1, 3], (-18.547146092248852, -17.928030266681613))\n",
      "([1, 5, 4, 1, 3, 4, 5, 3, 2, 3], (-19.325467801462263, -17.689203224408899))\n",
      "([1, 5, 4, 5, 3, 4, 5, 3, 2, 3], (-19.328748799764973, -17.694105969982637))\n",
      "([1, 5, 4, 1, 3, 4, 5, 3, 4], (-18.79699026165903, -17.945090229238392))\n",
      "([1, 5, 4, 5, 3, 4, 5, 3, 4], (-18.803585534273239, -17.95258394264377))\n",
      "([1, 5, 4, 3, 4, 3, 5, 2, 3], (-19.181531846082809, -17.859420073785095))\n",
      "([1, 5, 4, 1, 3, 4, 5, 2, 3, 2], (-19.439349296385199, -17.884502168470895))\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def prefix_beam_decode(y, beam_size=10, blank=0):\n",
    "    T, V = y.shape\n",
    "    log_y = np.log(y)\n",
    "\n",
    "    beam = [(tuple(), (0, ninf))]  # blank, non-blank\n",
    "    for t in range(T):  # for every timestep\n",
    "        new_beam = defaultdict(lambda: (ninf, ninf))\n",
    "\n",
    "        for prefix, (p_b, p_nb) in beam:\n",
    "            for i in range(V):  # for every state\n",
    "                p = log_y[t, i]\n",
    "\n",
    "                if i == blank:  # propose a blank\n",
    "                    new_p_b, new_p_nb = new_beam[prefix]\n",
    "                    new_p_b = logsumexp(new_p_b, p_b + p, p_nb + p)\n",
    "                    new_beam[prefix] = (new_p_b, new_p_nb)\n",
    "                    continue\n",
    "                else:  # extend with non-blank\n",
    "                    end_t = prefix[-1] if prefix else None\n",
    "\n",
    "                    # exntend current prefix\n",
    "                    new_prefix = prefix + (i,)\n",
    "                    new_p_b, new_p_nb = new_beam[new_prefix]\n",
    "                    if i != end_t:\n",
    "                        new_p_nb = logsumexp(new_p_nb, p_b + p, p_nb + p)\n",
    "                    else:\n",
    "                        new_p_nb = logsumexp(new_p_nb, p_b + p)\n",
    "                    new_beam[new_prefix] = (new_p_b, new_p_nb)\n",
    "\n",
    "                    # keep current prefix\n",
    "                    if i == end_t:\n",
    "                        new_p_b, new_p_nb = new_beam[prefix]\n",
    "                        new_p_nb = logsumexp(new_p_nb, p_nb + p)\n",
    "                        new_beam[prefix] = (new_p_b, new_p_nb)\n",
    "\n",
    "        # top beam_size\n",
    "        beam = sorted(new_beam.items(),\n",
    "                      key=lambda x: logsumexp(*x[1]), reverse=True)\n",
    "        beam = beam[:beam_size]\n",
    "\n",
    "    return beam\n",
    "\n",
    "\n",
    "np.random.seed(1111)\n",
    "y = softmax(np.random.random([20, 6]))\n",
    "beam = prefix_beam_decode(y, beam_size=100)\n",
    "for string, score in beam[:20]:\n",
    "    print(remove_blank(string), score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 其他解码方法\n",
    "上述介绍了基本解码方法。实际中，搜索过程可以接合额外的信息，提高搜索的准确度（例如在语音识别任务中，加入语言模型得分信息, [前缀搜索+语言模型](https://github.com/PaddlePaddle/DeepSpeech/blob/develop/decoders/decoders_deprecated.py\n",
    ")）。\n",
    "\n",
    "本质上，CTC 只是一个训练准则。训练完成后，$N_w$ 输出一系列概率分布，这点和常规基于交叉熵准则训练的模型完全一致。因此，特定应用领域常规的解码也可以经过一定修改用来 CTC 的解码。例如在语音识别任务中，利用 CTC 训练的声学模型可以无缝融入原来的 WFST 的解码器中[5]（e.g. 参见 [EESEN](https://github.com/srvk/eesen)）。\n",
    "\n",
    "此外，[1] 给出了一种利用 CTC 顶峰特点的启发式搜索方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 工具\n",
    "\n",
    "[warp-ctc](https://github.com/baidu-research/warp-ctc) 是百度开源的基于 CPU 和 GPU 的高效并行实现。warp-ctc 自身提供 C 语言接口，对于流利的机器学习工具（[torch](https://github.com/baidu-research/warp-ctc/tree/master/torch_binding)、 [pytorch](https://github.com/SeanNaren/deepspeech.pytorch) 和 [tensorflow](https://github.com/baidu-research/warp-ctc/tree/master/tensorflow_binding)、[chainer](https://github.com/jheymann85/chainer_ctc)）都有相应的接口绑定。\n",
    "\n",
    "[cudnn 7](https://developer.nvidia.com/cudnn) 以后开始提供 CTC 支持。\n",
    "\n",
    "Tensorflow 也原生支持 [CTC loss](https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss)，及 greedy 和 beam search 解码器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小结\n",
    "1. CTC 可以建模无对齐信息的多对多序列问题（输入长度不小于输出），如语音识别、连续字符识别 [3,4]。\n",
    "2. CTC 不需要输入与输出的对齐信息，可以实现端到端的训练。\n",
    "3. CTC 在 loss 的计算上，利用了整个 labels 序列的全局信息，某种意义上相对逐帧计算损失的方法，\"更加区分性\"。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Graves et al. [Connectionist Temporal Classification : Labelling Unsegmented Sequence Data with Recurrent Neural Networks](ftp://ftp.idsia.ch/pub/juergen/icml2006.pdf).\n",
    "2. Hannun et al. [First-Pass Large Vocabulary Continuous Speech Recognition using Bi-Directional Recurrent DNNs](https://arxiv.org/abs/1408.2873).\n",
    "3. Graves et al. [Towards End-To-End Speech Recognition with Recurrent Neural Networks](http://jmlr.org/proceedings/papers/v32/graves14.pdf).\n",
    "4. Liwicki et al. [A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks](https://www.cs.toronto.edu/~graves/icdar_2007.pdf).\n",
    "5. Zenkel et al. [Comparison of Decoding Strategies for CTC Acoustic Models](https://arxiv.org/abs/1708.004469).\n",
    "5. Huannun. [Sequence Modeling with CTC](https://distill.pub/2017/ctc/)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
